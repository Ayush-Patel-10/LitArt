{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# append a new directory to sys.path\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(__file__)))\n",
    "sys.path.append('/home/verma.shi/LLM/LitArt/data_module')\n",
    "sys.path.append('/home/verma.shi/LLM/LitArt/models')\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextSummaryDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 textprocessor,\n",
    "                 tokenizer,\n",
    "                 tokenizer_chapter_max_length=1024,\n",
    "                 tokenizer_summary_max_length=64,\n",
    "                 truncation=True,\n",
    "                 ):\n",
    "\n",
    "        self.df = df\n",
    "        self.textprocessor = textprocessor\n",
    "        self.chapter = df[\"chapter\"]\n",
    "        self.summary = df[\"summary_text\"]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_chapter_max_length = tokenizer_chapter_max_length\n",
    "        self.tokenizer_summary_max_length = tokenizer_summary_max_length\n",
    "        self.truncation = truncation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chapter)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        chapter = \"Summarize the following : \\n\" + str(self.textprocessor.process(self.chapter[idx])) + \"\\n\\nSummary:\"\n",
    "        summary = self.textprocessor.process(self.summary[idx])\n",
    "\n",
    "        input_encodings = self.tokenizer(chapter, max_length=self.tokenizer_chapter_max_length,padding=\"max_length\", truncation=self.truncation)\n",
    "\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(summary, max_length=self.tokenizer_summary_max_length,padding=\"max_length\", truncation=self.truncation)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_encodings[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(input_encodings[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(target_encodings[\"input_ids\"], dtype=torch.long),\n",
    "            \"summary_mask\": torch.tensor(target_encodings[\"attention_mask\"], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# append a new directory to sys.path\n",
    "# sys.path.append(os.path.dirname(os.path.dirname(__file__)))\n",
    "sys.path.append('/home/verma.shi/LLM/LitArt/data_module')\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "from data_preprocessor import TextPreprocessing\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class TextDataModule(L.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 train_path,\n",
    "                 test_path,\n",
    "                 val_path,\n",
    "                 textprocessor,\n",
    "                 tokenizer,\n",
    "                 tokenizer_chapter_max_length=1024,\n",
    "                 tokenizer_summary_max_length=64,\n",
    "                 truncation = True,\n",
    "                 batch_size: int = 32):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Initializing Paths\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.val_path = val_path\n",
    "\n",
    "        # Initializing Dataframes\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.val_df = None\n",
    "\n",
    "        # Textprocessor setup\n",
    "        self.textprocessor = textprocessor\n",
    "\n",
    "        # Tokenizer setup\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_chapter_max_length = tokenizer_chapter_max_length\n",
    "        self.tokenizer_summary_max_length = tokenizer_summary_max_length\n",
    "        self.truncation = truncation\n",
    "\n",
    "        # Batch size setup\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "         # Reading the train file\n",
    "        try:\n",
    "            self.train_df = pd.read_csv(self.train_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception raised while reading training file at path : {self.train_path} \\n Exception : {e}\")\n",
    "\n",
    "        # Reading the test file\n",
    "        try:\n",
    "            self.test_df = pd.read_csv(self.test_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception raised while reading test file at path : {self.test_path} \\n Exception : {e}\")\n",
    "\n",
    "        # Reading the validation file\n",
    "        try:\n",
    "            self.val_df = pd.read_csv(self.val_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception raised while reading validation file at path : {self.val_path} \\n Exception : {e}\")\n",
    "\n",
    "    def total_documents(self):\n",
    "        \n",
    "        total_documents = self.train_df.shape[0] + self.test_df.shape[0] + self.val_df.shape[0]\n",
    "\n",
    "        return total_documents\n",
    "\n",
    "\n",
    "    def setup(self, stage= None):\n",
    "        self.train_dataset = TextSummaryDataset(\n",
    "            df=self.train_df,\n",
    "            textprocessor=self.textprocessor,\n",
    "            tokenizer=self.tokenizer,\n",
    "            tokenizer_chapter_max_length=self.tokenizer_chapter_max_length,\n",
    "            tokenizer_summary_max_length=self.tokenizer_summary_max_length,\n",
    "            truncation=self.truncation)\n",
    "\n",
    "        self.val_dataset = TextSummaryDataset(\n",
    "            df=self.val_df,\n",
    "            textprocessor=self.textprocessor,\n",
    "            tokenizer=self.tokenizer,\n",
    "            tokenizer_chapter_max_length=self.tokenizer_chapter_max_length,\n",
    "            tokenizer_summary_max_length=self.tokenizer_summary_max_length,\n",
    "            truncation=self.truncation)\n",
    "\n",
    "        self.test_dataset = TextSummaryDataset(\n",
    "            df=self.test_df,\n",
    "            textprocessor=self.textprocessor,\n",
    "            tokenizer=self.tokenizer,\n",
    "            tokenizer_chapter_max_length=self.tokenizer_chapter_max_length,\n",
    "            tokenizer_summary_max_length=self.tokenizer_summary_max_length,\n",
    "            truncation=self.truncation)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextSummaryModel(L.LightningModule):\n",
    "    def __init__(self,model,\n",
    "                     total_documents = 5000,\n",
    "                     epochs=2):\n",
    "        super(TextSummaryModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.epochs = int(epochs)\n",
    "        self.total_documents = int(total_documents)\n",
    "        self.config = model.config\n",
    "        self.config = model.can_generate\n",
    "\n",
    "\n",
    "    def set_model(self,model):\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                attention_mask, \n",
    "                labels = None, \n",
    "                decoder_attention_mask = None):\n",
    "        \n",
    "        outputs = self.model(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             labels=labels,\n",
    "                             decoder_attention_mask=decoder_attention_mask)\n",
    "\n",
    "        return outputs.loss, outputs.logits\n",
    "\n",
    "    def training_step(self,batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        decoder_attention_mask = batch[\"summary_mask\"]\n",
    "\n",
    "        loss , output = self(input_ids = input_ids,\n",
    "                            attention_mask = attention_mask,\n",
    "                            labels = labels,\n",
    "                            decoder_attention_mask = decoder_attention_mask)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self , batch , batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        decoder_attention_mask = batch[\"summary_mask\"]\n",
    "\n",
    "        loss , output = self(input_ids = input_ids,\n",
    "                            attention_mask = attention_mask,\n",
    "                            labels = labels,\n",
    "                            decoder_attention_mask = decoder_attention_mask)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        decoder_attention_mask = batch[\"summary_mask\"]\n",
    "        loss, output = self(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            labels = labels,\n",
    "                            decoder_attention_mask = decoder_attention_mask)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=0.0001)\n",
    "        # scheduler = get_linear_schedule_with_warmup(\n",
    "        #         optimizer, num_warmup_steps=500,\n",
    "        #         num_training_steps=self.epochs*self.total_documents)\n",
    "        return {'optimizer': optimizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_details(path):\n",
    "\n",
    "    with open(path+\"run_config.json\") as json_file:\n",
    "        run_details = json.load(json_file)\n",
    "    \n",
    "    base_model_name = run_details[\"base_model_name\"]\n",
    "    tokenizer_name = run_details[\"tokenizer_name\"]\n",
    "    cache_dir = run_details[\"cache_dir\"]\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name,cache_dir=cache_dir).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,cache_dir=cache_dir)\n",
    "\n",
    "    checkpoint_location = path+\"my_model/version_0/checkpoints/*.ckpt\"\n",
    "    best_checkpoint_location = glob.glob(checkpoint_location)[0]\n",
    "\n",
    "    summary_model = TextSummaryModel.load_from_checkpoint(checkpoint_path=best_checkpoint_location,model=base_model)\n",
    "    summary_model.to(device)\n",
    "    summary_model.freeze()\n",
    "\n",
    "    run_details[\"best_model_path\"] = best_checkpoint_location\n",
    "    \n",
    "    return summary_model,base_model,tokenizer,run_details\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = \"/work/LitArt/verma/google-t5-t5-small-14:13:17/\"\n",
    "summary_model,base_model,tokenizer,run_details = load_model_details(checkpoints_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text,model,tokenizer,chapter_length):\n",
    "    model = model.to(device)\n",
    "    inputs = tokenizer(text, \n",
    "                       max_length=chapter_length,\n",
    "                       truncation=False,\n",
    "                       padding=\"max_length\",\n",
    "                       add_special_tokens=True, \n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    summarized_ids = model.model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"], \n",
    "        max_length= 64,        num_beams=4).to(device)\n",
    "\n",
    "    return \" \".join([tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "                    for token_ids in summarized_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    " Once upon a time, in the bustling streets of a vibrant city, there lived an elephant named Ellie. Ellie was a gentle giant most of the time, but when it came to navigating the chaotic traffic of the city, she had a bit of a temper.\n",
    "One sunny afternoon, Ellie was on her way to the market to pick up some fresh fruits and vegetables. As she ambled along the crowded streets, cars honked loudly, motorbikes whizzed by, and pedestrians darted in and out of traffic. Ellie did her best to stay calm, but the constant noise and chaos were starting to get to her.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ellie lives in the bustling streets of a vibrant city called Ellie. Ellie is a gentle giant most of the time, but when it comes to navigating the chaotic traffic of the city, there is an elephant named Ellie.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text,summary_model,tokenizer,chapter_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_pipeline = pipeline(\"summarization\",model=summary_model.model,tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_pipeline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
