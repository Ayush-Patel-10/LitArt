{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "tqdm.tqdm.pandas()\n",
    "sys.path.append('/home/verma.shi/LLM/LitArt/models')\n",
    "from summarizer import TextSummaryModel\n",
    "cache_dir=\"/work/LitArt/cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_details(path):\n",
    "\n",
    "    with open(path+\"run_config.json\") as json_file:\n",
    "        run_details = json.load(json_file)\n",
    "    \n",
    "    base_model_name = run_details[\"base_model_name\"]\n",
    "    tokenizer_name = run_details[\"tokenizer_name\"]\n",
    "    cache_dir = run_details[\"cache_dir\"]\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name,cache_dir=cache_dir).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,cache_dir=cache_dir)\n",
    "\n",
    "    checkpoint_location = path+\"my_model/version_0/checkpoints/*.ckpt\"\n",
    "    best_checkpoint_location = glob.glob(checkpoint_location)[0]\n",
    "\n",
    "    model = torch.load(f=best_checkpoint_location,map_location=device)\n",
    "    keys_to_modify = list(model[\"state_dict\"].keys())  # Create a copy of the keys\n",
    "    for key in keys_to_modify:\n",
    "        new_key = key[6:]\n",
    "        model[\"state_dict\"][new_key] = model[\"state_dict\"][key]\n",
    "        del model[\"state_dict\"][key]\n",
    "\n",
    "    summary_model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path=base_model_name,state_dict=model[\"state_dict\"])\n",
    "\n",
    "    run_details[\"best_model_path\"] = best_checkpoint_location\n",
    "    \n",
    "    return summary_model,base_model,tokenizer,run_details\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoints_path = \"/work/LitArt/verma/google-pegasus-xsum-2024-03-14-23:19:43/\"\n",
    "summary_model,base_model,tokenizer,run_details = load_model_details(checkpoints_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_path': '/work/LitArt/data/generated_summaries/train_dataset_with_summaries.csv',\n",
       " 'test_path': '/work/LitArt/data/generated_summaries/test_dataset_with_summaries.csv',\n",
       " 'val_path': '/work/LitArt/data/generated_summaries/validation_dataset_with_summaries.csv',\n",
       " 'base_model_name': 'google/pegasus-xsum',\n",
       " 'tokenizer_name': 'google/pegasus-xsum',\n",
       " 'cache_dir': '/work/LitArt/cache',\n",
       " 'batch_size': 32,\n",
       " 'tokenizer_chapter_max_length': 512,\n",
       " 'tokenizer_summary_max_length': 64,\n",
       " 'epochs': 10,\n",
       " 'log_path': '/work/LitArt/verma/',\n",
       " 'best_model_path': '/work/LitArt/verma/google-pegasus-xsum-2024-03-14-23:19:43/my_model/version_0/checkpoints/epoch=6-val_loss=3.82.ckpt'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text,model,tokenizer,chapter_length,summary_length,temperature=1,repetition_penalty=1,device='cpu'):\n",
    "    model = model.to(device)\n",
    "    text = \"Summarize the following : \\n\" + text\n",
    "    inputs = tokenizer(text, \n",
    "                       max_length=chapter_length,\n",
    "                       truncation=True,\n",
    "                       padding=\"max_length\",\n",
    "                       add_special_tokens=True, \n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    summarized_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"], \n",
    "            max_length= summary_length,\n",
    "            temperature = temperature,\n",
    "            do_sample = True,\n",
    "            repetition_penalty = repetition_penalty).to(device)\n",
    "\n",
    "    return \" \".join([tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "                    for token_ids in summarized_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(run_details[\"test_path\"])\n",
    "test_df = test_df.sample(n=100,random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:31<00:00,  4.51s/it]\n"
     ]
    }
   ],
   "source": [
    "test_df[\"model_summary\"] = test_df[\"chapter\"].progress_apply( lambda text: summarize(text,summary_model,tokenizer,chapter_length=run_details[\"tokenizer_chapter_max_length\"],summary_length=run_details[\"tokenizer_summary_max_length\"],temperature=1.5,repetition_penalty=1.5,device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [06:22<00:00,  3.82s/it]\n"
     ]
    }
   ],
   "source": [
    "test_df[\"base_model_summary\"] = test_df[\"chapter\"].progress_apply( lambda text: summarize(text,base_model,tokenizer,chapter_length=run_details[\"tokenizer_chapter_max_length\"],summary_length=run_details[\"tokenizer_summary_max_length\"],temperature=1.5,repetition_penalty=1.5,device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.1060802331681305,\n",
       " 'rouge2': 0.02020470364595954,\n",
       " 'rougeL': 0.07256178889223228,\n",
       " 'rougeLsum': 0.07310122951667639}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = test_df[\"model_summary\"].to_list()\n",
    "references = test_df[\"summary_text\"].to_list()\n",
    "results_model = rouge.compute(predictions=predictions, references=references)\n",
    "results_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.06966278793405115,\n",
       " 'rouge2': 0.013128496993345683,\n",
       " 'rougeL': 0.051979927863158755,\n",
       " 'rougeLsum': 0.05243213606150958}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = test_df[\"generated_summary\"].to_list()\n",
    "references = test_df[\"summary_text\"].to_list()\n",
    "results_gpt = rouge.compute(predictions=predictions, references=references)\n",
    "results_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.0722585722639571,\n",
       " 'rouge2': 0.01058775230274284,\n",
       " 'rougeL': 0.05240994838149399,\n",
       " 'rougeLsum': 0.05264777377620605}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = test_df[\"base_model_summary\"].to_list()\n",
    "references = test_df[\"summary_text\"].to_list()\n",
    "results_base = rouge.compute(predictions=predictions, references=references)\n",
    "results_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_difference(dict1, dict2):\n",
    "    percentage_difference = {}\n",
    "\n",
    "    for metric in dict1.keys():\n",
    "        difference = dict2[metric] - dict1[metric]\n",
    "        percentage_diff = (difference / dict1[metric]) * 100\n",
    "        percentage_difference[metric] = percentage_diff\n",
    "\n",
    "    for metric, percentage_diff in percentage_difference.items():\n",
    "        print(f\"{metric}: {percentage_diff:.2f}%\")\n",
    "\n",
    "    return percentage_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 52.28%\n",
      "rouge2: 53.90%\n",
      "rougeL: 39.60%\n",
      "rougeLsum: 39.42%\n"
     ]
    }
   ],
   "source": [
    "percentage_change = calculate_percentage_difference(results_gpt,results_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: 46.81%\n",
      "rouge2: 90.83%\n",
      "rougeL: 38.45%\n",
      "rougeLsum: 38.85%\n"
     ]
    }
   ],
   "source": [
    "percentage_change = calculate_percentage_difference(results_base,results_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "chapter = test_df.iloc[index][\"chapter\"]\n",
    "summary = test_df.iloc[index][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_summary = summarize(chapter,\n",
    "                               base_model,\n",
    "                               tokenizer,\n",
    "                               chapter_length=run_details[\"tokenizer_chapter_max_length\"],\n",
    "                               summary_length=run_details[\"tokenizer_summary_max_length\"],\n",
    "                               temperature=1.5,\n",
    "                               repetition_penalty=1.5,\n",
    "                               device=device)\n",
    "model_summary = summarize(chapter,\n",
    "                               summary_model,\n",
    "                               tokenizer,\n",
    "                               chapter_length=run_details[\"tokenizer_chapter_max_length\"],\n",
    "                               summary_length=run_details[\"tokenizer_summary_max_length\"],\n",
    "                               temperature=1.5,\n",
    "                               repetition_penalty=1.5,\n",
    "                               device=device)\n",
    "gpt_summary = test_df.iloc[index][\"generated_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'says my idealistic friend what vulgar details what good is there in taking all these pains to give an exact likeness of old women and clowns what a low phase of life what clumsy ugly people but bless us things may be lovable that are not altogether handsome i hope i am not at all sure that the majority of the human race have not been ugly and even among those lords of their kind the british squat figures ill-shapen nostrils and dingy complexions are not startling exceptions yet there is a great deal of family love amongst us i have a friend or two whose class of features is such that the apollo curl on the summit of their brows would be decidedly trying yet to my certain knowledge tender hearts have beaten for them and their miniatures--flattering but still not lovely--are kissed in secret by motherly lips i have seen many an excellent matron who could have never in her best days have been handsome and yet she had a packet of yellow love-letters in a private drawer and sweet children showered kisses on her sallow cheeks and i believe there have been plenty of young heroes of middle stature and feeble beards who have felt quite sure they could never love anything more insignificant than a diana and yet have found themselves in middle life happily settled with a wife who waddles yes thank god human feeling is like the mighty rivers that bless the earth it does not wait for beauty--it flows with resistless force and brings beauty with it all honour and reverence to the divine beauty of form let us cultivate it to the utmost in men women and children--in our gardens and in our houses but let us love that other beauty too which lies in no secret of proportion but in the secret of deep human sympathy paint us an angel if you can with a floating violet robe and a face paled by the celestial light paint us yet oftener a madonna turning her mild face upward and opening her arms to welcome the divine glory but do not impose on us any aesthetic rules which shall banish from the region of art those old women scraping carrots with their work-worn hands those heavy clowns taking holiday in a dingy pot-house those rounded backs and stupid weather-beaten faces that have bent over the spade and done the rough work of the world--those homes with their tin pans their brown pitchers their rough curs and their clusters of onions in this world there are so many of these common coarse people who have no picturesque sentimental wretchedness it is so needful we should remember their existence else we may happen to leave them quite out of our religion and philosophy and frame lofty theories which only fit a world of extremes therefore let art always remind us of them therefore let us always have men ready to give the loving pains of a life to the faithful representing of commonplace things--men who see beauty in these commonplace things and delight in showing how kindly the light of heaven falls on them there are few prophets in the world few sublimely beautiful women few heroes i can t afford to give all my love and reverence to such rarities i want a great deal of those feelings for my every-day fellow-men especially for the few in the foreground of the great multitude whose faces i know whose hands i touch for whom i have to make way with kindly courtesy neither are picturesque lazzaroni or romantic criminals half so frequent as your common labourer who gets his own bread and eats it vulgarly but creditably with his own pocket-knife it is more needful that i should have a fibre of sympathy connecting me with that vulgar citizen who weighs out my sugar in a vilely assorted cravat and waistcoat than with the handsomest rascal in red scarf and green feathers--more needful that my heart should swell with loving admiration at some trait of gentle goodness in the faulty people who sit at the same hearth with me or in the clergyman of my own parish who is perhaps rather too corpulent and in other respects is not an oberlin or a tillotson than at the deeds of heroes whom i shall never know except by hearsay or at the sublimest abstract of all clerical graces that was ever conceived by an able novelist and so i come back to mr irwine with whom i desire you to be in perfect charity far as he may be from satisfying your demands on the clerical character perhaps you think he was not--as he ought to have been--a living demonstration of the benefits attached to a national church but i am not sure of that at least i know that the people in broxton and hayslope would have been very sorry to part with their clergyman and that most faces brightened at his approach and until it can be proved that hatred is a better thing for the soul than love i must believe that mr irwine s influence in his parish was a more wholesome one than that of the zealous mr ryde who came there twenty years afterwards when mr irwine had been gathered to his fathers it is true mr ryde insisted strongly on the doctrines of the reformation visited his flock a great deal in their own homes and was severe in rebuking the aberrations of the flesh--put a stop indeed to the christmas rounds of the church singers as promoting drunkenness and too light a handling of sacred things but i gathered from adam bede to whom i talked of these matters in his old age that few clergymen could be less successful in winning the hearts of their parishioners than mr ryde they learned a great many notions about doctrine from him so that almost every church-goer under fifty began'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Summary : \n",
      " My dear friend what is it about the human race that makes us so ugly?\n",
      "Fine Tuned Model Summary : \n",
      " in the second act jane tells her idealistic friend that there are some people in the human race who have not been ugly and even among those lords of their kind the majority of the human race has not been ugly and even among those lords of their kind the apollo curl on the summit of their brows would\n",
      "GPT Model Summary : \n",
      " The passage emphasizes the importance of human connection over aesthetic beauty, highlighting the value of common, everyday people and experiences.\n",
      "Human Summary : \n",
      " book second chapter in which the story pauses a little this chapter is a time out from the plot as eliot steps in as the author to explain her style of storytelling she defends the realism of it and eschews being sentimental or using ideal character types she is writing from nature and fact as if she were in a courtroom she cannot lie or whitewash the details she reminds us that we must accept the flawed human beings around us the real breathing men and women for these are the only ones who can be helped and encouraged she compares her storytelling to dutch painting known for its homely and realistic detail instead of the beautiful gentry as her subjects she would rather write about the village wedding with old people peasants with large noses and vulgar ways there is family love among them and beauty of outward form is not as important as the beauty of human sympathy an example of this kind of beauty is found in mr irwine who did not preach informative sermons on doctrine but was good to his parishioners she reminds the reader that the events of the story happened sixty years ago before pastors were overzealous mr ryde who succeeded mr irwine was better known for preaching doctrine but he was not loved as mr irwine was she has asked the old adam bede about these things and he has told her that religion is not notions but the emotion that helps people do the right thing the author concludes that human nature is lovable as it is love and heroism as we picture them don t exist as depicted in romances\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base Model Summary : \\n {base_model_summary}\")\n",
    "print(f\"Fine Tuned Model Summary : \\n {model_summary}\")\n",
    "print(f\"GPT Model Summary : \\n {gpt_summary}\")\n",
    "print(f\"Human Summary : \\n {summary}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
