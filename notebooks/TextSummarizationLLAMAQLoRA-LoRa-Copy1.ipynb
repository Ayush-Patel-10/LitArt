{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from datetime import date\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "#Transformers\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from transformers import AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "#Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "#PEFT\n",
    "from peft import LoraConfig\n",
    "from peft import PeftConfig\n",
    "from peft import PeftModel\n",
    "from peft import get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to print the number of trainable parameters in the given model\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable %: {100 * trainable_params / all_param}\")\n",
    "\n",
    "def tokenize_input(df,tokenizer,tokenizer_chapter_max_length,tokenizer_summary_max_length):\n",
    "\n",
    "    prompt_start = \"Summarize the following : \\n\"\n",
    "    prompt_end = \"\\n\\nSummary:\"\n",
    "\n",
    "    prompt = [prompt_start + dialogue + prompt_end for dialogue in df[\"chapter\"]]\n",
    "\n",
    "    df[\"input_ids\"] = tokenizer(prompt, max_length=tokenizer_chapter_max_length , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "    df[\"labels\"] = tokenizer(df[\"summary_text\"],max_length=tokenizer_summary_max_length , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/work/LitArt/cache\" \n",
    "log_path = \"/work/LitArt/verma/\"\n",
    "\n",
    "tokenizer_chapter_max_length = 1024\n",
    "tokenizer_summary_max_length = 128\n",
    "base_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "#Training Parameters\n",
    "batch_size = 2\n",
    "epochs = 1\n",
    "log_path = log_path+base_model_name.replace(\"/\",\"-\")+\"-\" +str(today)+\"-\"+time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "logger = TensorBoardLogger(log_path, name=\"my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b15287ad004af8a7f443baf4a131a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 4718592 || All params: 3613463424 || Trainable %: 0.13058363808693696\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "cache_dir = \"/work/LitArt/cache\" \n",
    "\n",
    "#Bits and Bytes config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "load_in_4bit=True, #4bit quantizaition - load_in_4bit is used to load models in 4-bit quantization \n",
    "bnb_4bit_use_double_quant=True, #nested quantization technique for even greater memory efficiency without sacrificing performance. This technique has proven beneficial, especially when fine-tuning large models\n",
    "bnb_4bit_quant_type=\"nf4\", #quantization type used is 4 bit Normal Float Quantization- The NF4 data type is designed for weights initialized using a normal distribution\n",
    "bnb_4bit_compute_dtype=torch.bfloat16, #modify the data type used during computation. This can result in speed improvements. \n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name,\n",
    "                                                    device_map=\"auto\",\n",
    "                                                    trust_remote_code=True,\n",
    "                                                    quantization_config=bnb_config,\n",
    "                                                    cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "\n",
    "# Enable gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce the memory consumption during the backward pas. Instead of storing all intermediate activations in the forward pass (which is what's typically done to compute gradients in the backward pass), gradient checkpointing stores only a subset of them\n",
    "base_model.gradient_checkpointing_enable() \n",
    "\n",
    "# Prepare the model for k-bit training . Applies some preprocessing to the model to prepare it for training.\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "#If only targeting attention blocks of the model\n",
    "#target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "#If targeting all linear layers\n",
    "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config)\n",
    "\n",
    "#base_model = get_peft_model(base_model, config)\n",
    "\n",
    "# Print the number of trainable parameters in the model\n",
    "print_trainable_parameters(base_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('csv', \n",
    "                    data_files={\n",
    "                        'train': \"/work/LitArt/data/chunked_dataset/train_dataset_with_summaries.csv\",\n",
    "                        'test': \"/work/LitArt/data/chunked_dataset/test_dataset_with_summaries.csv\",\n",
    "                        'val':\"/work/LitArt/data/chunked_dataset/validation_dataset_with_summaries.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 10668\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 1614\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 1548\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "'''\n",
    "Dataset({\n",
    "    features: ['instruction', 'input', 'output'],\n",
    "    num_rows: 20022\n",
    "})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d0bd5d67ca45ebbd36b07ae91c0234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = data[\"train\"].shuffle().map(tokenize_input, batched=True, fn_kwargs={\"tokenizer\": tokenizer, \"tokenizer_chapter_max_length\": tokenizer_chapter_max_length,\"tokenizer_summary_max_length\":tokenizer_summary_max_length})\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['chapter', 'human_summary', '__index_level_0__', 'summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "YOUR_HF_USERNAME = \"rohitnair212\"\n",
    "\n",
    "output_dir = f\"{YOUR_HF_USERNAME}/llama-7b-qlora-Capstone-project\"\n",
    "per_device_train_batch_size = batch_size\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\" #\"paged_adamw_8bit\"\n",
    "save_steps = 10\n",
    "save_total_limit=3\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps =200 #1000\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\" #\"cosine\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=log_path,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    #save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    #save_strategy='epoch',\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_checkpointing=True,\n",
    "    #push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### USER: {example['chapter']}\\n### ASSISTANT: {example['summary_text']}\"\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=data[\"train\"],\n",
    "    packing=True,\n",
    "    #dataset_text_field=\"id\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=1024,\n",
    "    formatting_func=formatting_func,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save_pretrained(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/LitArt/verma/tiiuae-falcon-7b-2024-03-18-20:27:30'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m \u001b[43mlog_path\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_path' is not defined"
     ]
    }
   ],
   "source": [
    "model_dir = log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=quantization_config,\n",
    "    adapter_kwargs={\"revision\": \"09487e6ffdcc75838b10b6138b6149c36183164e\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:20\u001b[0;36m\u001b[0m\n\u001b[0;31m    with torch.no_grad():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_response(chapter : str) -> str:\n",
    "\tprompt =  f\"\"\"\n",
    "    ### USER:\" {chapter}\n",
    "    ### Assistant: \n",
    "    \"\"\".strip()\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "    outputs = model.generate(inputs.input_ids, max_new_tokens=250, do_sample=False)\n",
    "    return(tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "\n",
    "\n",
    "'''\n",
    "\tencoding = tokenizer(prompt, return_tensors = \"pt\").to(DEVICE)\n",
    "\t#with torch.inference_mode():\n",
    "    with torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\tinput_ids=encoding.input_ids,\n",
    "\t\t\tattention_mask=encoding.attention_mask,\n",
    "\t\t\tgeneration_config=generation_config,\n",
    "\t\t)\n",
    "\n",
    "\tresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t#assistant_start =  \"<assistant>:\"\n",
    "\t#response_start = response.find(assistant_start)\n",
    "\t#return response[response_start + len(assistant_start) : ].strip()\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "chapter = \"text to be summarised\"\n",
    "print (generate_response(chapter))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration for the trained model\n",
    "config = PeftConfig.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model using the loaded configuration and other parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig.base_model_name_or_path,\n",
    "\treturn_dict=True,\n",
    "\tquantization_config=bnb_config,\n",
    "\tdevice_map=\"auto\",\n",
    "\ttrust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(chapter : str) -> str:\n",
    "    \n",
    "    \n",
    "\tprompt =  f\"\"\"\n",
    "    \"Summarize the following : \\n\" {chapter}\n",
    "    \\n\\nSummary: \n",
    "    \"\"\".strip()\n",
    "\tencoding = tokenizer(prompt, return_tensors = \"pt\").to(DEVICE)\n",
    "\t#with torch.inference_mode():\n",
    "    with torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\tinput_ids=encoding.input_ids,\n",
    "\t\t\tattention_mask=encoding.attention_mask,\n",
    "\t\t\tgeneration_config=generation_config,\n",
    "\t\t)\n",
    "\n",
    "\tresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t#assistant_start =  \"<assistant>:\"\n",
    "\t#response_start = response.find(assistant_start)\n",
    "\t#return response[response_start + len(assistant_start) : ].strip()\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt\n",
    "\n",
    "chapter = \"text to be summarised\"\n",
    "print (generate_response(chapter))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
