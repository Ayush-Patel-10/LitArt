{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d956d69e-c7c3-467a-a605-fa3d487aec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "from data_preprocessor import TextPreprocessing\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "import re\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class TextPreprocessing():\n",
    "    def __init__(self ,\n",
    "                 regexList = None,\n",
    "                 punct= True,\n",
    "                 lowercase= True,\n",
    "                 slang= False,\n",
    "                 stopwordList = None,\n",
    "                 stemming = False,\n",
    "                 lemmatization= False ):\n",
    "\n",
    "        self.convertToLowercase = lowercase #Done\n",
    "        self.removePunctuations = punct #Done\n",
    "        self.regexList = regexList  # Done\n",
    "        self.removeSlang = slang #Done\n",
    "        self.stopwordList = stopwordList #Done\n",
    "        self.useStemming = stemming #Done\n",
    "        self.useLemmatization = lemmatization #Done\n",
    "\n",
    "    def process(self , text):\n",
    "        # Make text lower case\n",
    "        if self.convertToLowercase:\n",
    "            text = text.lower()\n",
    "\n",
    "        pattern = r\"\\s*\\([a-zA-Z]\\s_\\)\"\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "\n",
    "        #Convert multiline with spaces\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "        if self.removeSlang:\n",
    "            text = contractions.fix(text)\n",
    "\n",
    "        #Remove punctuations\n",
    "        if self.removePunctuations:\n",
    "            text = re.sub(r\"[=.!,¿?.!+,;¿/:|%()<>।॰{}#_'\\\"@$^&*']\", \" \", text)\n",
    "            text = re.sub(r\"…\", \" \", text)\n",
    "\n",
    "        # remove double quotes\n",
    "        text = re.sub(r'\"', \" \", text)\n",
    "\n",
    "        # remove numbers\n",
    "        text = re.sub(r'[0-9]', \"\", text)\n",
    "        # sentence = re.sub(r'#([^s]+)', r'1', sentence)\n",
    "\n",
    "        # remove website links\n",
    "        text = re.sub('((www.[^s]+)|(https?://[^s]+))', '', text)\n",
    "\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r'[\" \"]+', \" \", text)\n",
    "\n",
    "        # remove extra space\n",
    "        text = text.strip()\n",
    "\n",
    "        if self.regexList is not None:\n",
    "            for regex in self.regexList:\n",
    "                text = re.sub(regex, '', text)\n",
    "\n",
    "        if self.stopwordList is not None:\n",
    "            text_list = text.split()\n",
    "            text_list = [word for word in text_list if word not in self.stopwordList]\n",
    "            text = \" \".join(text_list)\n",
    "\n",
    "        #Stemming (convert the word into root word)\n",
    "        if self.useStemming:\n",
    "            ps = nltk.stem.porter.PorterStemmer()\n",
    "            text_list = text.split()\n",
    "            text_list = [ps.stem(word) for word in text_list]\n",
    "            text = \" \".join(text_list)\n",
    "\n",
    "        #Lemmatization (convert the word into root word)\n",
    "        if self.useLemmatization:\n",
    "            lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "            text_list = text.split()\n",
    "            text_list = [lem.lemmatize(word) for word in text_list]\n",
    "            text = \" \".join(text_list)\n",
    "\n",
    "        return text\n",
    "\n",
    "class TextSummaryDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 textprocessor,\n",
    "                 tokenizer,\n",
    "                 tokenizer_chapter_max_length=1024,\n",
    "                 tokenizer_summary_max_length=64,\n",
    "                 truncation=True,\n",
    "                 ):\n",
    "\n",
    "        self.df = df\n",
    "        self.textprocessor = textprocessor\n",
    "        self.chapter = df[\"chapter\"]\n",
    "        self.summary = df[\"summary_text\"]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_chapter_max_length = tokenizer_chapter_max_length\n",
    "        self.tokenizer_summary_max_length = tokenizer_summary_max_length\n",
    "        self.truncation = truncation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chapter)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        chapter = \"summarize:\" + str(self.textprocessor.process(self.chapter[idx]))\n",
    "        summary = self.textprocessor.process(self.summary[idx])\n",
    "\n",
    "        input_encodings = self.tokenizer(chapter, max_length=self.tokenizer_chapter_max_length,padding=\"max_length\", truncation=self.truncation)\n",
    "\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(summary, max_length=self.tokenizer_summary_max_length,padding=\"max_length\", truncation=self.truncation)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_encodings[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(input_encodings[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(target_encodings[\"input_ids\"], dtype=torch.long),\n",
    "            \"summary_mask\": torch.tensor(target_encodings[\"attention_mask\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class TextDataModule(L.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 train_path,\n",
    "                 test_path,\n",
    "                 val_path,\n",
    "                 textprocessor,\n",
    "                 tokenizer,\n",
    "                 tokenizer_chapter_max_length=1024,\n",
    "                 tokenizer_summary_max_length=64,\n",
    "                 truncation = True,\n",
    "                 batch_size: int = 32):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Initializing Paths\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.val_path = val_path\n",
    "\n",
    "        # Initializing Dataframes\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.val_df = None\n",
    "\n",
    "        # Textprocessor setup\n",
    "        self.textprocessor = textprocessor\n",
    "\n",
    "        # Tokenizer setup\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_chapter_max_length = tokenizer_chapter_max_length\n",
    "        self.tokenizer_summary_max_length = tokenizer_summary_max_length\n",
    "        self.truncation = truncation\n",
    "\n",
    "        # Batch size setup\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "         # Reading the train file\n",
    "        try:\n",
    "            self.train_df = pd.read_csv(self.train_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception raised while reading training file at path : {self.train_path} \\n Exception : {e}\")\n",
    "\n",
    "        # Reading the test file\n",
    "        try:\n",
    "            self.test_df = pd.read_csv(self.test_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception raised while reading test file at path : {self.test_path} \\n Exception : {e}\")\n",
    "\n",
    "        # Reading the validation file\n",
    "        try:\n",
    "            self.val_df = pd.read_csv(self.val_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception raised while reading validation file at path : {self.val_path} \\n Exception : {e}\")\n",
    "\n",
    "\n",
    "    def setup(self, stage= None):\n",
    "        self.train_dataset = TextSummaryDataset(\n",
    "            df=self.train_df,\n",
    "            textprocessor=self.textprocessor,\n",
    "            tokenizer=self.tokenizer,\n",
    "            tokenizer_chapter_max_length=self.tokenizer_chapter_max_length,\n",
    "            tokenizer_summary_max_length=self.tokenizer_summary_max_length,\n",
    "            truncation=self.truncation)\n",
    "\n",
    "        self.val_dataset = TextSummaryDataset(\n",
    "            df=self.val_df,\n",
    "            textprocessor=self.textprocessor,\n",
    "            tokenizer=self.tokenizer,\n",
    "            tokenizer_chapter_max_length=self.tokenizer_chapter_max_length,\n",
    "            tokenizer_summary_max_length=self.tokenizer_summary_max_length,\n",
    "            truncation=self.truncation)\n",
    "\n",
    "        self.test_dataset = TextSummaryDataset(\n",
    "            df=self.test_df,\n",
    "            textprocessor=self.textprocessor,\n",
    "            tokenizer=self.tokenizer,\n",
    "            tokenizer_chapter_max_length=self.tokenizer_chapter_max_length,\n",
    "            tokenizer_summary_max_length=self.tokenizer_summary_max_length,\n",
    "            truncation=self.truncation)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "class ImageDataModule(L.LightningModule):\n",
    "    def __init__(self, data_dir: str = \"path/to/dir\", batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        ## Image Data\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.traindataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valdataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.testdataset, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de0595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12515\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../Datasets/Training_data.csv\")\n",
    "test_df = pd.read_csv(\"../Datasets/Testing_data.csv\")\n",
    "val_df = pd.read_csv(\"../Datasets/Validation_data.csv\")\n",
    "total_documents = len(train_df)+len(test_df)+len(val_df)\n",
    "print(total_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c892ec-7794-43bf-bc55-f876e26e2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummaryModel(L.LightningModule):\n",
    "    def __init__(self,model,\n",
    "                     epochs=2):\n",
    "        super(TextSummaryModel,self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                attention_mask, \n",
    "                labels = None, \n",
    "                decoder_attention_mask = None):\n",
    "        \n",
    "        outputs = self.model(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             labels=labels,\n",
    "                             decoder_attention_mask=decoder_attention_mask)\n",
    "\n",
    "        return outputs.loss, outputs.logits\n",
    "\n",
    "    def training_step(self,batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        decoder_attention_mask = batch[\"summary_mask\"]\n",
    "\n",
    "        loss , output = self(input_ids = input_ids,\n",
    "                            attention_mask = attention_mask,\n",
    "                            labels = labels,\n",
    "                            decoder_attention_mask = decoder_attention_mask)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self , batch , batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        decoder_attention_mask = batch[\"summary_mask\"]\n",
    "\n",
    "        loss , output = self(input_ids = input_ids,\n",
    "                            attention_mask = attention_mask,\n",
    "                            labels = labels,\n",
    "                            decoder_attention_mask = decoder_attention_mask)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        loss, output = self(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=0.0001)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=0,\n",
    "                num_training_steps=epochs*total_documents)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f1a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "# Model and Tokenizer Setup\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Training files setup\n",
    "train_path = \"../Datasets/Training_data.csv\"\n",
    "test_path = \"../Datasets/Testing_data.csv\"\n",
    "val_path = \"../Datasets/Validation_data.csv\"\n",
    "\n",
    "\n",
    "# Text Preprocessor setup\n",
    "textpreprocessor = TextPreprocessing()\n",
    "\n",
    "textmodule = TextDataModule(train_path=train_path,\n",
    "                                     val_path=val_path,\n",
    "                                     test_path=test_path,\n",
    "                                     textprocessor=textpreprocessor,\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     tokenizer_chapter_max_length=1024,\n",
    "                                     tokenizer_summary_max_length=64,\n",
    "                                     truncation=True)\n",
    "textmodule.prepare_data()\n",
    "textmodule.setup()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Setting up data\n",
    "batch_size = 4\n",
    "chapter_length = 512\n",
    "summary_length = 64\n",
    "epochs = 2\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs = epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "model = TextSummaryModel(model=model_t5,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d21fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception raised while reading training file at path : ../Datasets/Training_data.csv \n",
      " Exception : Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31b54a321cb4edb9c239f6658d93c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, textmodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae03d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
