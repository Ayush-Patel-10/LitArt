{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d956d69e-c7c3-467a-a605-fa3d487aec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from data_preprocessor import TextPreprocessing\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "import re\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class TextPreprocessing():\n",
    "    def __init__(self ,\n",
    "                 regexList = None,\n",
    "                 punct= True,\n",
    "                 lowercase= True,\n",
    "                 slang= False,\n",
    "                 stopwordList = None,\n",
    "                 stemming = False,\n",
    "                 lemmatization= False ):\n",
    "\n",
    "        self.convertToLowercase = lowercase #Done\n",
    "        self.removePunctuations = punct #Done\n",
    "        self.regexList = regexList  # Done\n",
    "        self.removeSlang = slang #Done\n",
    "        self.stopwordList = stopwordList #Done\n",
    "        self.useStemming = stemming #Done\n",
    "        self.useLemmatization = lemmatization #Done\n",
    "\n",
    "    def process(self , text):\n",
    "        # Make text lower case\n",
    "        if self.convertToLowercase:\n",
    "            text = text.lower()\n",
    "\n",
    "        pattern = r\"\\s*\\([a-zA-Z]\\s_\\)\"\n",
    "        text = re.sub(pattern, \"\", text)\n",
    "\n",
    "        #Convert multiline with spaces\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "        if self.removeSlang:\n",
    "            text = contractions.fix(text)\n",
    "\n",
    "        #Remove punctuations\n",
    "        if self.removePunctuations:\n",
    "            text = re.sub(r\"[=.!,¿?.!+,;¿/:|%()<>।॰{}#_'\\\"@$^&*']\", \" \", text)\n",
    "            text = re.sub(r\"…\", \" \", text)\n",
    "\n",
    "        # remove double quotes\n",
    "        text = re.sub(r'\"', \" \", text)\n",
    "\n",
    "        # remove numbers\n",
    "        text = re.sub(r'[0-9]', \"\", text)\n",
    "        # sentence = re.sub(r'#([^s]+)', r'1', sentence)\n",
    "\n",
    "        # remove website links\n",
    "        text = re.sub('((www.[^s]+)|(https?://[^s]+))', '', text)\n",
    "\n",
    "        # remove multiple spaces\n",
    "        text = re.sub(r'[\" \"]+', \" \", text)\n",
    "\n",
    "        # remove extra space\n",
    "        text = text.strip()\n",
    "\n",
    "        if self.regexList is not None:\n",
    "            for regex in self.regexList:\n",
    "                text = re.sub(regex, '', text)\n",
    "\n",
    "        if self.stopwordList is not None:\n",
    "            text_list = text.split()\n",
    "            text_list = [word for word in text_list if word not in self.stopwordList]\n",
    "            text = \" \".join(text_list)\n",
    "\n",
    "        #Stemming (convert the word into root word)\n",
    "        if self.useStemming:\n",
    "            ps = nltk.stem.porter.PorterStemmer()\n",
    "            text_list = text.split()\n",
    "            text_list = [ps.stem(word) for word in text_list]\n",
    "            text = \" \".join(text_list)\n",
    "\n",
    "        #Lemmatization (convert the word into root word)\n",
    "        if self.useLemmatization:\n",
    "            lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "            text_list = text.split()\n",
    "            text_list = [lem.lemmatize(word) for word in text_list]\n",
    "            text = \" \".join(text_list)\n",
    "\n",
    "        return text\n",
    "\n",
    "class TextSummaryDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 textprocessor,\n",
    "                 tokenizer,\n",
    "                 tokenizer_chapter_max_length=1024,\n",
    "                 tokenizer_summary_max_length=64,\n",
    "                 truncation=True,\n",
    "                 ):\n",
    "\n",
    "        self.df = df\n",
    "        self.textprocessor = textprocessor\n",
    "        self.chapter = df[\"chapter\"]\n",
    "        self.summary = df[\"summary_text\"]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_chapter_max_length = tokenizer_chapter_max_length\n",
    "        self.tokenizer_summary_max_length = tokenizer_summary_max_length\n",
    "        self.truncation = truncation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chapter)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        chapter = \"summarize:\" + str(self.textprocessor.process(self.chapter[idx]))\n",
    "        summary = self.textprocessor.process(self.summary[idx])\n",
    "\n",
    "        input_encodings = self.tokenizer(chapter, max_length=self.tokenizer_chapter_max_length,padding=\"max_length\", truncation=self.truncation)\n",
    "\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(summary, max_length=self.tokenizer_summary_max_length,padding=\"max_length\", truncation=self.truncation)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_encodings[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(input_encodings[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(target_encodings[\"input_ids\"], dtype=torch.long),\n",
    "            \"summary_mask\": torch.tensor(target_encodings[\"attention_mask\"], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class TextDataModule(L.LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 train_path,\n",
    "                 test_path,\n",
    "                 val_path,\n",
    "                 textprocessor,\n",
    "                 tokenizer,\n",
    "                 tokenizer_chapter_max_length=1024,\n",
    "                 tokenizer_summary_max_length=64,\n",
    "                 truncation = True,\n",
    "                 batch_size: int = 32):\n",
    "\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Initializing Paths\n",
    "        self.train_path = train_path\n",
    "        self.test_path = test_path\n",
    "        self.val_path = val_path\n",
    "\n",
    "        # Initializing Dataframes\n",
    "        self.train_df = None\n",
    "        self.test_df = None\n",
    "        self.val_df = None\n",
    "\n",
    "        # Textprocessor setup\n",
    "        self.textprocessor = textprocessor\n",
    "\n",
    "        # Tokenizer setup\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_chapter_max_length = tokenizer_chapter_max_length\n",
    "        self.tokenizer_summary_max_length = tokenizer_summary_max_length\n",
    "        self.truncation = truncation\n",
    "\n",
    "        # Batch size setup\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "         # Reading the train file\n",
    "        try:\n",
    "            self.train_df = pd.read_csv(self.train_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception raised while reading training file at path : {self.train_path} \\n Exception : {e}\")\n",
    "\n",
    "        # Reading the test file\n",
    "        try:\n",
    "            self.test_df = pd.read_csv(self.test_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception raised while reading test file at path : {self.test_path} \\n Exception : {e}\")\n",
    "\n",
    "        # Reading the validation file\n",
    "        try:\n",
    "            self.val_df = pd.read_csv(self.val_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception raised while reading validation file at path : {self.val_path} \\n Exception : {e}\")\n",
    "\n",
    "\n",
    "    def setup(self, stage= None):\n",
    "        self.train_dataset = TextSummaryDataset(\n",
    "            df=self.train_df,\n",
    "            textprocessor=self.textprocessor,\n",
    "            tokenizer=self.tokenizer,\n",
    "            tokenizer_chapter_max_length=self.tokenizer_chapter_max_length,\n",
    "            tokenizer_summary_max_length=self.tokenizer_summary_max_length,\n",
    "            truncation=self.truncation)\n",
    "\n",
    "        self.val_dataset = TextSummaryDataset(\n",
    "            df=self.val_df,\n",
    "            textprocessor=self.textprocessor,\n",
    "            tokenizer=self.tokenizer,\n",
    "            tokenizer_chapter_max_length=self.tokenizer_chapter_max_length,\n",
    "            tokenizer_summary_max_length=self.tokenizer_summary_max_length,\n",
    "            truncation=self.truncation)\n",
    "\n",
    "        self.test_dataset = TextSummaryDataset(\n",
    "            df=self.test_df,\n",
    "            textprocessor=self.textprocessor,\n",
    "            tokenizer=self.tokenizer,\n",
    "            tokenizer_chapter_max_length=self.tokenizer_chapter_max_length,\n",
    "            tokenizer_summary_max_length=self.tokenizer_summary_max_length,\n",
    "            truncation=self.truncation)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "class ImageDataModule(L.LightningModule):\n",
    "    def __init__(self, data_dir: str = \"path/to/dir\", batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        ## Image Data\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.traindataset, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valdataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.testdataset, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de0595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12515\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../Datasets/Training_data.csv\")\n",
    "test_df = pd.read_csv(\"../Datasets/Testing_data.csv\")\n",
    "val_df = pd.read_csv(\"../Datasets/Validation_data.csv\")\n",
    "total_documents = len(train_df)+len(test_df)+len(val_df)\n",
    "\n",
    "print(total_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c892ec-7794-43bf-bc55-f876e26e2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummaryModel(L.LightningModule):\n",
    "    def __init__(self,model,\n",
    "                     epochs=2):\n",
    "        super(TextSummaryModel,self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def set_model(self,model):\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                attention_mask, \n",
    "                labels = None, \n",
    "                decoder_attention_mask = None):\n",
    "        \n",
    "        outputs = self.model(input_ids=input_ids,\n",
    "                             attention_mask=attention_mask,\n",
    "                             labels=labels,\n",
    "                             decoder_attention_mask=decoder_attention_mask)\n",
    "\n",
    "        return outputs.loss, outputs.logits\n",
    "\n",
    "    def training_step(self,batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        decoder_attention_mask = batch[\"summary_mask\"]\n",
    "\n",
    "        loss , output = self(input_ids = input_ids,\n",
    "                            attention_mask = attention_mask,\n",
    "                            labels = labels,\n",
    "                            decoder_attention_mask = decoder_attention_mask)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self , batch , batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        decoder_attention_mask = batch[\"summary_mask\"]\n",
    "\n",
    "        loss , output = self(input_ids = input_ids,\n",
    "                            attention_mask = attention_mask,\n",
    "                            labels = labels,\n",
    "                            decoder_attention_mask = decoder_attention_mask)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        decoder_attention_mask = batch[\"summary_mask\"]\n",
    "        loss, output = self(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask,\n",
    "                            labels = labels,\n",
    "                            decoder_attention_mask = decoder_attention_mask)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=0.0001)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=0,\n",
    "                num_training_steps=epochs*total_documents)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5f1a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Model and Tokenizer Setup\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Training files setup\n",
    "train_path = \"../Datasets/Training_data.csv\"\n",
    "test_path = \"../Datasets/Testing_data.csv\"\n",
    "val_path = \"../Datasets/Validation_data.csv\"\n",
    "\n",
    "\n",
    "# Text Preprocessor setup\n",
    "textpreprocessor = TextPreprocessing()\n",
    "\n",
    "textmodule = TextDataModule(train_path=train_path,\n",
    "                                     val_path=val_path,\n",
    "                                     test_path=test_path,\n",
    "                                     textprocessor=textpreprocessor,\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     tokenizer_chapter_max_length=1024,\n",
    "                                     tokenizer_summary_max_length=64,\n",
    "                                     truncation=True)\n",
    "textmodule.prepare_data()\n",
    "textmodule.setup()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Setting up data\n",
    "batch_size = 4\n",
    "chapter_length = 512\n",
    "summary_length = 64\n",
    "epochs = 1\n",
    "\n",
    "log_path = \"/work/LitArt/verma/lightning_logs\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filename=\"{epoch}-{val_loss:.2f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True,\n",
    "    save_top_k=1,\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    callbacks=[\n",
    "                checkpoint_callback,\n",
    "            ],\n",
    "    max_epochs = epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    # max_stepxs = 5,\n",
    "    devices=1,\n",
    "    default_root_dir = log_path\n",
    ")\n",
    "\n",
    "model = TextSummaryModel(model=model_t5,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8d21fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /work/LitArt/verma/lightning_logs/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f771c65920415fbe39b0b190f28425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65900b7d616e40b18375123efcfd6d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0054008d0b07473aa0f048697931482c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 300: 'val_loss' reached 3.07552 (best 3.07552), saving model to '/work/LitArt/verma/lightning_logs/lightning_logs/version_0/checkpoints/epoch=0-val_loss=3.08.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, textmodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f87211e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/LitArt/verma/lightning_logs/lightning_logs/version_0/checkpoints/epoch=0-val_loss=3.08.ckpt'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = checkpoint_callback.best_model_path\n",
    "best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /work/LitArt/verma/lightning_logs/lightning_logs/version_0/checkpoints/epoch=0-val_loss=3.08.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /work/LitArt/verma/lightning_logs/lightning_logs/version_0/checkpoints/epoch=0-val_loss=3.08.ckpt\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c72e4c02a424e74b7984c8f39e76000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/LitArt/verma/capstone/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(\n",
    "    model=model,\n",
    "    datamodule=textmodule,\n",
    "    ckpt_path=best_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dae03d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text,model,tokenizer,chapter_length):\n",
    "    model = model.to(device)\n",
    "    inputs = tokenizer(text, \n",
    "                       max_length=chapter_length,\n",
    "                       truncation=False,\n",
    "                       padding=\"max_length\",\n",
    "                       add_special_tokens=True, \n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    summarized_ids = model.model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"], \n",
    "        max_length= 64,        num_beams=10).to(device)\n",
    "\n",
    "    return \" \".join([tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "                    for token_ids in summarized_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7ceccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    " Once upon a time, in the bustling streets of a vibrant city, there lived an elephant named Ellie. Ellie was a gentle giant most of the time, but when it came to navigating the chaotic traffic of the city, she had a bit of a temper.\n",
    "\n",
    "One sunny afternoon, Ellie was on her way to the market to pick up some fresh fruits and vegetables. As she ambled along the crowded streets, cars honked loudly, motorbikes whizzed by, and pedestrians darted in and out of traffic. Ellie did her best to stay calm, but the constant noise and chaos were starting to get to her.\n",
    "\n",
    "Suddenly, a small car cut her off, nearly brushing against her side. Ellie let out a loud trumpet of annoyance, her patience wearing thin. The driver of the car, a young man with a careless smirk on his face, paid no heed to Ellie's displeasure.\n",
    "\n",
    "Feeling indignant, Ellie decided she'd had enough. With a determined glint in her eye, she carefully maneuvered herself around the car, her massive body blocking its path. The driver's smirk quickly faded as he realized he was at the mercy of an angry elephant.\n",
    "\n",
    "\"Hey, watch where you're going, you big oaf!\" the driver shouted, pounding his fists against the steering wheel.\n",
    "\n",
    "But Ellie was unfazed. She stood her ground, refusing to move until the driver offered a sincere apology. Traffic came to a standstill as bystanders watched in amazement at the spectacle unfolding before them.\n",
    "\n",
    "After a few tense moments, the driver begrudgingly muttered an apology, his face flushed with embarrassment. Ellie nodded her head in acceptance and gracefully stepped aside, allowing the traffic to flow once again.\n",
    "\n",
    "As Ellie continued on her way to the market, she couldn't help but feel a sense of satisfaction. Sometimes, a little road rage was necessary to remind others to show respect and courtesy, even to the largest of creatures. And from that day on, drivers in the city made sure to give Ellie plenty of space on the road, knowing that they wouldn't want to incur the wrath of an elephant again.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "304aac6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that Ellie was a gentle giant most of the time, but when it came to navigating the chaotic traffic of the city, there was an elephant named Ellie. Ellie was a gentle giant most of the time, but when it came to navigating the chaotic traffic of the city,'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text,model,tokenizer,chapter_length=chapter_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f0cf27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/LitArt/verma/lightning_logs/lightning_logs/version_0/checkpoints/epoch=0-val_loss=3.08.ckpt'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e1769e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t5 = model_t5.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d03e57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_summary_model = TextSummaryModel.load_from_checkpoint(checkpoint_path=best_model_path,model=model_t5)\n",
    "news_summary_model.to(device)\n",
    "news_summary_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "10950ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that Ellie was a gentle giant most of the time, but when it came to navigating the chaotic traffic of the city, there was an elephant named Ellie. Ellie was a gentle giant most of the time, but when it came to navigating the chaotic traffic of the city,'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text,news_summary_model,tokenizer,chapter_length=chapter_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
