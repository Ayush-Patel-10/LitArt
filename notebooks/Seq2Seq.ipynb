{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ba532c8-9f17-4d99-a0ef-c6c474dbbfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/patel.ayushj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "#Import libraries for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "#Import libraries for text processing\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "#Import the pytorch libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c2c483-a041-4e12-859e-fb5fa5039e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Training_data.csv')\n",
    "validate_data = pd.read_csv('Validation_data.csv')\n",
    "test_data = pd.read_csv('Testing_data.csv')\n",
    "\n",
    "train_data = train_data[['summary_id','chapter','chapter_length','summary_name','summary_text','summary_analysis','summary_length','analysis_length']]\n",
    "validate_data = validate_data[['summary_id','chapter','chapter_length','summary_name','summary_text','summary_analysis','summary_length','analysis_length']]\n",
    "test_data = test_data[['summary_id','chapter','chapter_length','summary_name','summary_text','summary_analysis','summary_length','analysis_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed383cd-eaf9-4181-ac1f-c6bb1f5484ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>chapter</th>\n",
       "      <th>chapter_length</th>\n",
       "      <th>summary_name</th>\n",
       "      <th>summary_text</th>\n",
       "      <th>summary_analysis</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>analysis_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapters 1-2</td>\n",
       "      <td>\\n  \"Mine ear is open, and my heart prepared:\\...</td>\n",
       "      <td>6471.0</td>\n",
       "      <td>Chapters 1-2</td>\n",
       "      <td>Before any characters appear, the time and geo...</td>\n",
       "      <td>These two chapters introduce the reader to the...</td>\n",
       "      <td>388.0</td>\n",
       "      <td>473.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chapter 3</td>\n",
       "      <td>\\n  \"Before these fields were shorn and tilled...</td>\n",
       "      <td>3132.0</td>\n",
       "      <td>Chapter 3</td>\n",
       "      <td>In another part of the forest by the river a f...</td>\n",
       "      <td>This chapter introduces the other three main a...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chapter 4</td>\n",
       "      <td>\\n  \"Well, go thy way: thou shalt not from thi...</td>\n",
       "      <td>3075.0</td>\n",
       "      <td>Chapter 4</td>\n",
       "      <td>When the mounted party from Fort Howard approa...</td>\n",
       "      <td>Since this chapter is mostly one of surface ac...</td>\n",
       "      <td>319.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chapter 5</td>\n",
       "      <td>\\n                      \"In such a night\\n  Di...</td>\n",
       "      <td>3268.0</td>\n",
       "      <td>Chapter 5</td>\n",
       "      <td>The pursuit of Magua is unsuccessful, but Hawk...</td>\n",
       "      <td>Here the reader encounters the first bloodshed...</td>\n",
       "      <td>329.0</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chapter 6</td>\n",
       "      <td>\\n  \"Those strains that once did sweet in Zion...</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>Chapter 6</td>\n",
       "      <td>Heyward and the girls are uneasy and Gamut is ...</td>\n",
       "      <td>This chapter shows Cooper in his most inventiv...</td>\n",
       "      <td>321.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     summary_id                                            chapter  \\\n",
       "0  chapters 1-2  \\n  \"Mine ear is open, and my heart prepared:\\...   \n",
       "1     chapter 3  \\n  \"Before these fields were shorn and tilled...   \n",
       "2     chapter 4  \\n  \"Well, go thy way: thou shalt not from thi...   \n",
       "3     chapter 5  \\n                      \"In such a night\\n  Di...   \n",
       "4     chapter 6  \\n  \"Those strains that once did sweet in Zion...   \n",
       "\n",
       "   chapter_length  summary_name  \\\n",
       "0          6471.0  Chapters 1-2   \n",
       "1          3132.0     Chapter 3   \n",
       "2          3075.0     Chapter 4   \n",
       "3          3268.0     Chapter 5   \n",
       "4          3873.0     Chapter 6   \n",
       "\n",
       "                                        summary_text  \\\n",
       "0  Before any characters appear, the time and geo...   \n",
       "1  In another part of the forest by the river a f...   \n",
       "2  When the mounted party from Fort Howard approa...   \n",
       "3  The pursuit of Magua is unsuccessful, but Hawk...   \n",
       "4  Heyward and the girls are uneasy and Gamut is ...   \n",
       "\n",
       "                                    summary_analysis  summary_length  \\\n",
       "0  These two chapters introduce the reader to the...           388.0   \n",
       "1  This chapter introduces the other three main a...           198.0   \n",
       "2  Since this chapter is mostly one of surface ac...           319.0   \n",
       "3  Here the reader encounters the first bloodshed...           329.0   \n",
       "4  This chapter shows Cooper in his most inventiv...           321.0   \n",
       "\n",
       "   analysis_length  \n",
       "0            473.0  \n",
       "1            149.0  \n",
       "2             75.0  \n",
       "3            156.0  \n",
       "4            128.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0de1db1-3e68-4874-9e16-7a3e6adc8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    #lower text\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    pattern = r\"\\s*\\([a-zA-Z]\\s_\\)\"\n",
    "    sentence = re.sub(pattern, \"\", sentence)\n",
    "\n",
    "    sentence = sentence.replace(\"\\n\",\" \")\n",
    "\n",
    "    # replacing everything with space\n",
    "    sentence = re.sub(r\"[=.!,¿?.!+,;¿/:|%()<>।॰{}#_'\\\"@$^&*']\", \" \", sentence)\n",
    "    sentence = re.sub(r\"…\", \" \", sentence)\n",
    "\n",
    "    #remove double quotes\n",
    "    sentence = re.sub(r'\"', \" \", sentence)\n",
    "\n",
    "    #remove numbers\n",
    "    sentence = re.sub(r'[0-9]', \"\", sentence)\n",
    "    #sentence = re.sub(r'#([^s]+)', r'1', sentence)\n",
    "\n",
    "    #remove website links\n",
    "    sentence = re.sub('((www.[^s]+)|(https?://[^s]+))','',sentence)\n",
    "\n",
    "    #remove @anythin here\n",
    "    #sentence = re.sub('@[^s]+','',sentence)\n",
    "\n",
    "    #remove multiple spaces\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # remove extra space\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9595b2d0-0a99-41bb-bf95-2d9c58d5c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['summary_text'] = train_data['summary_text'].apply(lambda x: pre_processing(x))\n",
    "train_data['chapter'] = train_data['chapter'].apply(lambda x: pre_processing(x))\n",
    "\n",
    "validate_data['summary_text'] = validate_data['summary_text'].apply(lambda x: pre_processing(x))\n",
    "validate_data['chapter'] = validate_data['chapter'].apply(lambda x: pre_processing(x))\n",
    "\n",
    "test_data['summary_text'] = test_data['summary_text'].apply(lambda x: pre_processing(x))\n",
    "test_data['chapter'] = test_data['chapter'].apply(lambda x: pre_processing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d78ebda-1320-4a08-913f-3c1bce77d0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'before any characters appear the time and geography are made clear though it is the last war that england and france waged for a country that neither would retain the wilderness between the forces still has to be overcome first thus it is in in the new york area between the head waters of the hudson river and lake george to the north because only two years earlier general braddock was disgracefully routed by a handful of french and indians the frontier is now exposed to real and imaginary savage disasters as well as to the horrors of warfare fear has replaced reason near dusk of a day in july an indian runner named magua arrives at fort edward on the upper hudson he has come from fort william henry at the southern tip of lake george with the news that the french general montcalm is moving south with a very large army and that munro commander of fort william henry is in urgent need of plentiful reinforcements from general webb early the next morning a limited detachment of fifteen hundred regulars and colonists departs as if swallowed by the forest shortly afterwards major duncan heyward and alice and cora munro guided by magua on foot take by horseback a secret route toward william henry for the girls to join their father blonde alice is doubtful about magua covered with war paint and showing a sullen fierceness but dark-haired cora is stoically common sense about him even though heyward mentions that their father had once had to deal rigidly with the indian as the small party pushes on they are overtaken by david gamut a tall ungainly psalmodist ridiculously dressed and carrying a pitch pipe while riding a mare followed by its young colt he desires to join them and after some banter between him and alice he pulls out the twenty-sixth edition of the bay psalm book sounds his pipe and renders a song in full sweet and melodious tones at a muttered comment from magua heyward insists upon silence for safety then he glances about them and satisfied that he has seen only shining berries smiles to himself as they move on but he is wrong the branches move and a man peers exultingly after them as they disappear among the dark lines of trees'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['summary_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba093b5-cd63-4e47-812b-aa497de4982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        ''' Add every word in a sentence to the vocabulary '''\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        ''' Add a word to the vocabulary'''\n",
    "        if word not in self.word2index:\n",
    "            #Include the word in the mapping from word to index\n",
    "            self.word2index[word] = self.n_words\n",
    "            #Set the count of ocurrencies of the word to 1\n",
    "            self.word2count[word] = 1\n",
    "            # Include the word in the indexes\n",
    "            self.index2word[self.n_words] = word\n",
    "            # Increment by 1 the number of words\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def save_to_file(self, filename):\n",
    "        ''' Save the Vocab object to a file'''\n",
    "        with open(filename,'wb') as f:\n",
    "            pickle.dump(self,f) \n",
    "\n",
    "def load_vocab(filename):\n",
    "    ''' Load a Vocab instance from a file'''\n",
    "    with open(filename,'rb') as f:\n",
    "        v = pickle.load(f)\n",
    "    return v\n",
    "\n",
    "def read_vocabs(text, summary, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Vocab(summary)\n",
    "        output_lang = Vocab(text)\n",
    "    else:\n",
    "        input_lang = Vocab(text)\n",
    "        output_lang = Vocab(summary)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_vocabs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff25976-e2a4-4c84-b1ae-1040d3804622",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data['chapter']\n",
    "y_train = train_data['summary_text']\n",
    "\n",
    "x_validate = validate_data['chapter']\n",
    "y_validate = validate_data['summary_text']\n",
    "\n",
    "x_test = test_data['chapter']\n",
    "y_test = test_data['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ca6a280-a2fb-495f-96b1-d6aa10129c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 9600 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "0       mine ear is open and my heart prepared the wor...\n",
      "1       before these fields were shorn and tilled full...\n",
      "2       well go thy way thou shalt not from this grove...\n",
      "3       in such a night did thisbe fearfully o ertrip ...\n",
      "4       those strains that once did sweet in zion glid...\n",
      "                              ...                        \n",
      "9595    there was a train for turin and paris that eve...\n",
      "9596    it was not with surprise it was with a feeling...\n",
      "9597    isabel s arrival at gardencourt on this second...\n",
      "9598    he had told her the first evening she ever spe...\n",
      "9599    the life and death of scyld the famous race of...\n",
      "Name: chapter, Length: 9600, dtype: object 168053\n",
      "0       before any characters appear the time and geog...\n",
      "1       in another part of the forest by the river a f...\n",
      "2       when the mounted party from fort howard approa...\n",
      "3       the pursuit of magua is unsuccessful but hawke...\n",
      "4       heyward and the girls are uneasy and gamut is ...\n",
      "                              ...                        \n",
      "9595    before isabel leaves rome she goes to see pans...\n",
      "9596    isabel is greeted by henrietta stackpole at ch...\n",
      "9597    isabel arrives at gardencourt the house is ver...\n",
      "9598    isabel remembers that when she first came to g...\n",
      "9599    beowulf begins with the legends of the warrior...\n",
      "Name: summary_text, Length: 9600, dtype: object 48368\n",
      "['in due time mr micawber s petition was ripe for hearing and that gentleman was ordered to be discharged under the act to my great joy his creditors were not implacable and mrs micawber informed me that even the revengeful boot-maker had declared in open court that he bore him no malice but that when money was owing to him he liked to be paid he said he thought it was human nature mr micawber returned to the king s bench when his case was over as some fees were to be settled and some formalities observed before he could be actually released the club received him with transport and held an harmonic meeting that evening in his honour while mrs micawber and i had a lamb s fry in private surrounded by the sleeping family on such an occasion i will give you master copperfield said mrs micawber in a little more flip for we had been having some already the memory of my papa and mama are they dead ma am i inquired after drinking the toast in a wine-glass my mama departed this life said mrs micawber before mr micawber s difficulties commenced or at least before they became pressing my papa lived to bail mr micawber several times and then expired regretted by a numerous circle mrs micawber shook her head and dropped a pious tear upon the twin who happened to be in hand as i could hardly hope for a more favourable opportunity of putting a question in which i had a near interest i said to mrs micawber may i ask ma am what you and mr micawber intend to do now that mr micawber is out of his difficulties and at liberty have you settled yet my family said mrs micawber who always said those two words with an air though i never could discover who came under the denomination my family are of opinion that mr micawber should quit london and exert his talents in the country mr micawber is a man of great talent master copperfield i said i was sure of that of great talent repeated mrs micawber my family are of opinion that with a little interest something might be done for a man of his ability in the custom house the influence of my family being local it is their wish that mr micawber should go down to plymouth they think it indispensable that he should be upon the spot that he may be ready i suggested exactly returned mrs micawber that he may be ready--in case of anything turning up and do you go too ma am the events of the day in combination with the twins if not with the flip had made mrs micawber hysterical and she shed tears as she replied i never will desert mr micawber mr micawber may have concealed his difficulties from me in the first instance but his sanguine temper may have led him to expect that he would overcome them the pearl necklace and bracelets which i inherited from mama have been disposed of for less than half their value and the set of coral which was the wedding gift of my papa has been actually thrown away for nothing but i never will desert mr micawber no cried mrs micawber more affected than before i never will do it it s of no use asking me i felt quite uncomfortable--as if mrs micawber supposed i had asked her to do anything of the sort --and sat looking at her in alarm mr micawber has his faults i do not deny that he is improvident i do not deny that he has kept me in the dark as to his resources and his liabilities both she went on looking at the wall but i never will desert mr micawber mrs micawber having now raised her voice into a perfect scream i was so frightened that i ran off to the club-room and disturbed mr micawber in the act of presiding at a long table and leading the chorus of gee up dobbin gee ho dobbin gee up dobbin gee up and gee ho--o--o with the tidings that mrs micawber was in an alarming state upon which he immediately burst into tears and came away with me with his waistcoat full of the heads and tails of shrimps of which he had been partaking emma my angel cried mr micawber running into the room what is the matter i never will desert you micawber she exclaimed my life said mr micawber taking her in his arms i am perfectly aware of it he is the parent of my children he is the father of my twins he is the husband of my affections cried mrs micawber struggling and i ne--ver--will--desert mr micawber mr micawber was so deeply affected by this proof of her devotion as to me i was dissolved in tears that he hung over her in a passionate manner imploring her to look up and to be calm but the more he asked mrs micawber to look up the more she fixed her eyes on nothing and the more he asked her to compose herself the more she wouldn t consequently mr micawber was soon so overcome that he mingled his tears with hers and mine until he begged me to do him the favour of taking a chair on the staircase while he got her into bed i would have taken my leave for the night but he would not hear of my doing that until the strangers bell should ring so i sat at the staircase window until he came out with another chair and joined me how is mrs micawber now sir i said very low said mr micawber shaking his head reaction ah this has been a dreadful day we stand alone now--everything is gone from us mr micawber pressed my hand and groaned and afterwards shed tears i was greatly touched and disappointed too for i had expected that we should be quite gay on this happy and long-looked-for occasion but mr and mrs micawber were so used to their old difficulties i think that they felt quite shipwrecked when they came to consider that they were released from them all their elasticity was departed and i never saw them half so wretched as on this night insomuch that when the bell rang and mr micawber walked with me to the lodge and parted from me there with a blessing i felt quite afraid to leave him by himself he was so profoundly miserable but through all the confusion and lowness of spirits in which we had been so unexpectedly to me involved i plainly discerned that mr and mrs micawber and their family were going away from london and that a parting between us was near at hand it was in my walk home that night and in the sleepless hours which followed when i lay in bed that the thought first occurred to me--though i don t know how it came into my head--which afterwards shaped itself into a settled resolution i had grown to be so accustomed to the micawbers and had been so intimate with them in their distresses and was so utterly friendless without them that the prospect of being thrown upon some new shift for a lodging and going once more among unknown people was like being that moment turned adrift into my present life with such a knowledge of it ready made as experience had given me all the sensitive feelings it wounded so cruelly all the shame and misery it kept alive within my breast became more poignant as i thought of this and i determined that the life was unendurable that there was no hope of escape from it unless the escape was my own act i knew quite well i rarely heard from miss murdstone and never from mr murdstone but two or three parcels of made or mended clothes had come up for me consigned to mr quinion and in each there was a scrap of paper to the effect that j m trusted d c was applying himself to business and devoting himself wholly to his duties--not the least hint of my ever being anything else than the common drudge into which i was fast settling down the very next day showed me while my mind was in the first agitation of what it had conceived that mrs micawber had not spoken of their going away without warrant they took a lodging in the house where i lived for a week at the expiration of which time they were to start for plymouth mr micawber himself came down to the counting-house in the afternoon to tell mr quinion that he must relinquish me on the day of his departure and to give me a high character which i am sure i deserved and mr quinion calling in tipp the carman who was a married man and had a room to let quartered me prospectively on him--by our mutual consent as he had every reason to think for i said nothing though my resolution was now taken i passed my evenings with mr and mrs micawber during the remaining term of our residence under the same roof and i think we became fonder of one another as the time went on on the last sunday they invited me to dinner and we had a loin of pork and apple sauce and a pudding i had bought a spotted wooden horse over-night as a parting gift to little wilkins micawber--that was the boy--and a doll for little emma i had also bestowed a shilling on the orfling who was about to be disbanded we had a very pleasant day though we were all in a tender state about our approaching separation i shall never master copperfield said mrs micawber revert to the period when mr micawber was in difficulties without thinking of you your conduct has always been of the most delicate and obliging description you have never been a lodger you have been a friend my dear said mr micawber copperfield for so he had been accustomed to call me of late has a heart to feel for the distresses of his fellow-creatures when they are behind a cloud and a head to plan and a hand to--in short a general ability to dispose of such available property as could be made away with i expressed my sense of this commendation and said i was very sorry we were going to lose one another my dear young friend said mr micawber i am older than you a man of some experience in life and--and of some experience in short in difficulties generally speaking at present and until something turns up which i am i may say hourly expecting i have nothing to bestow but advice still my advice is so far worth taking that--in short that i have never taken it myself and am the --here mr micawber who had been beaming and smiling all over his head and face up to the present moment checked himself and frowned-- the miserable wretch you behold my dear micawber urged his wife i say returned mr micawber quite forgetting himself and smiling again the miserable wretch you behold my advice is never do tomorrow what you can do today procrastination is the thief of time collar him my poor papa s maxim mrs micawber observed my dear said mr micawber your papa was very well in his way and heaven forbid that i should disparage him take him for all in all we ne er shall--in short make the acquaintance probably of anybody else possessing at his time of life the same legs for gaiters and able to read the same description of print without spectacles but he applied that maxim to our marriage my dear and that was so far prematurely entered into in consequence that i never recovered the expense mr micawber looked aside at mrs micawber and added not that i am sorry for it quite the contrary my love after which he was grave for a minute or so my other piece of advice copperfield said mr micawber you know annual income twenty pounds annual expenditure nineteen nineteen and six result happiness annual income twenty pounds annual expenditure twenty pounds ought and six result misery the blossom is blighted the leaf is withered the god of day goes down upon the dreary scene and--and in short you are for ever floored as i am to make his example the more impressive mr micawber drank a glass of punch with an air of great enjoyment and satisfaction and whistled the college hornpipe i did not fail to assure him that i would store these precepts in my mind though indeed i had no need to do so for at the time they affected me visibly next morning i met the whole family at the coach office and saw them with a desolate heart take their places outside at the back master copperfield said mrs micawber god bless you i never can forget all that you know and i never would if i could copperfield said mr micawber farewell every happiness and prosperity if in the progress of revolving years i could persuade myself that my blighted destiny had been a warning to you i should feel that i had not occupied another man s place in existence altogether in vain in case of anything turning up of which i am rather confident i shall be extremely happy if it should be in my power to improve your prospects i think as mrs micawber sat at the back of the coach with the children and i stood in the road looking wistfully at them a mist cleared from her eyes and she saw what a little creature i really was i think so because she beckoned to me to climb up with quite a new and motherly expression in her face and put her arm round my neck and gave me just such a kiss as she might have given to her own boy i had barely time to get down again before the coach started and i could hardly see the family for the handkerchiefs they waved it was gone in a minute the orfling and i stood looking vacantly at each other in the middle of the road and then shook hands and said good-bye she going back i suppose to st luke s workhouse as i went to begin my weary day at murdstone and grinby s but with no intention of passing many more weary days there no i had resolved to run away ---to go by some means or other down into the country to the only relation i had in the world and tell my story to my aunt miss betsey i have already observed that i don t know how this desperate idea came into my brain but once there it remained there and hardened into a purpose than which i have never entertained a more determined purpose in my life i am far from sure that i believed there was anything hopeful in it but my mind was thoroughly made up that it must be carried into execution again and again and a hundred times again since the night when the thought had first occurred to me and banished sleep i had gone over that old story of my poor mother s about my birth which it had been one of my great delights in the old time to hear her tell and which i knew by heart my aunt walked into that story and walked out of it a dread and awful personage but there was one little trait in her behaviour which i liked to dwell on and which gave me some faint shadow of encouragement i could not forget how my mother had thought that she felt her touch her pretty hair with no ungentle hand and though it might have been altogether my mother s fancy and might have had no foundation whatever in fact i made a little picture out of it of my terrible aunt relenting towards the girlish beauty that i recollected so well and loved so much which softened the whole narrative it is very possible that it had been in my mind a long time and had gradually engendered my determination as i did not even know where miss betsey lived i wrote a long letter to peggotty and asked her incidentally if she remembered pretending that i had heard of such a lady living at a certain place i named at random and had a curiosity to know if it were the same in the course of that letter i told peggotty that i had a particular occasion for half a guinea and that if she could lend me that sum until i could repay it i should be very much obliged to her and would tell her afterwards what i had wanted it for peggotty s answer soon arrived and was as usual full of affectionate devotion she enclosed the half guinea i was afraid she must have had a world of trouble to get it out of mr barkis s box and told me that miss betsey lived near dover but whether at dover itself at hythe sandgate or folkestone she could not say one of our men however informing me on my asking him about these places that they were all close together i deemed this enough for my object and resolved to set out at the end of that week being a very honest little creature and unwilling to disgrace the memory i was going to leave behind me at murdstone and grinby s i considered myself bound to remain until saturday night and as i had been paid a week s wages in advance when i first came there not to present myself in the counting-house at the usual hour to receive my stipend for this express reason i had borrowed the half-guinea that i might not be without a fund for my travelling-expenses accordingly when the saturday night came and we were all waiting in the warehouse to be paid and tipp the carman who always took precedence went in first to draw his money i shook mick walker by the hand asked him when it came to his turn to be paid to say to mr quinion that i had gone to move my box to tipp s and bidding a last good night to mealy potatoes ran away my box was at my old lodging over the water and i had written a direction for it on the back of one of our address cards that we nailed on the casks master david to be left till called for at the coach office dover this i had in my pocket ready to put on the box after i should have got it out of the house and as i went towards my lodging i looked about me for someone who would help me to carry it to the booking-office there was a long-legged young man with a very little empty donkey-cart standing near the obelisk in the blackfriars road whose eye i caught as i was going by and who addressing me as sixpenn orth of bad ha pence hoped i should know him agin to swear to --in allusion i have no doubt to my staring at him i stopped to assure him that i had not done so in bad manners but uncertain whether he might or might not like a job wot job said the long-legged young man to move a box i answered wot box said the long-legged young man i told him mine which was down that street there and which i wanted him to take to the dover coach office for sixpence done with you for a tanner said the long-legged young man and directly got upon his cart which was nothing but a large wooden tray on wheels and rattled away at such a rate that it was as much as i could do to keep pace with the donkey there was a defiant manner about this young man and particularly about the way in which he chewed straw as he spoke to me that i did not much like as the bargain was made however i took him upstairs to the room i was leaving and we brought the box down and put it on his cart now i was unwilling to put the direction-card on there lest any of my landlord s family should fathom what i was doing and detain me so i said to the young man that i would be glad if he would stop for a minute when he came to the dead-wall of the king s bench prison the words were no sooner out of my mouth than he rattled away as if he my box the cart and the donkey were all equally mad and i was quite out of breath with running and calling after him when i caught him at the place appointed being much flushed and excited i tumbled my half-guinea out of my pocket in pulling the card out i put it in my mouth for safety and though my hands trembled a good deal had just tied the card on very much to my satisfaction when i felt myself violently chucked under the chin by the long-legged young man and saw my half-guinea fly out of my mouth into his hand wot said the young man seizing me by my jacket collar with a frightful grin this is a pollis case is it you re a-going to bolt are you come to the pollis you young warmin come to the pollis you give me my money back if you please said i very much frightened and leave me alone come to the pollis said the young man you shall prove it yourn to the pollis give me my box and money will you i cried bursting into tears the young man still replied come to the pollis and was dragging me against the donkey in a violent manner as if there were any affinity between that animal and a magistrate when he changed his mind jumped into the cart sat upon my box and exclaiming that he would drive to the pollis straight rattled away harder than ever i ran after him as fast as i could but i had no breath to call out with and should not have dared to call out now if i had i narrowly escaped being run over twenty times at least in half a mile now i lost him now i saw him now i lost him now i was cut at with a whip now shouted at now down in the mud now up again now running into somebody s arms now running headlong at a post at length confused by fright and heat and doubting whether half london might not by this time be turning out for my apprehension i left the young man to go where he would with my box and money and panting and crying but never stopping faced about for greenwich which i had understood was on the dover road taking very little more out of the world towards the retreat of my aunt miss betsey than i had brought into it on the night when my arrival gave her so much umbrage', 'liking life on my own account no better i form a great resolution mr micawber is released from jail under the insolvent debtors act which forces him to surrender all his property the family decides to move to plymouth to look for work though mrs micawber insists i never will desert mr micawber she reveals that she is under great strain because of his improvidence and his habit of hiding his financial liabilities from her david cannot bear the thought of staying in london without the micawbers who have become his only friends as he bids them goodbye he resolves to throw himself on the mercy of his only relation betsey trotwood he sets out for dover employing a young man with a cart to take his belongings to the dover coach office the young man steals his money and his belongings and david is left to make his way to dover with nothing but the clothes he is wearing']\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabularies of the inout and output data and return the data in pairs of (source text, summary)\n",
    "input_lang, output_lang, pairs = prepare_data( x_train, y_train , False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97dd7a4c-7b1f-42c1-8ef2-e8dc54789bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: Maximum chapter length = 111988 , Maximum summary length = 4875\n",
      "Validation data: Maximum chapter length = 111988 , Maximum summary length = 4875\n",
      "Test data: Maximum chapter length = 111988 , Maximum summary length = 4875\n",
      "Overall maximum length for training data: 111989\n",
      "Overall maximum length for validation data: 111989\n",
      "Overall maximum length for test data: 111989\n",
      "111989\n"
     ]
    }
   ],
   "source": [
    "# For training data\n",
    "x_train = train_data['chapter']\n",
    "y_train = train_data['summary_text']\n",
    "max_chapter_length_train = max(len(chapter.split(' ')) for chapter in x_train)\n",
    "max_summary_length_train = max(len(summary.split(' ')) for summary in y_train)\n",
    "\n",
    "# For validation data\n",
    "x_validate = validate_data['chapter']\n",
    "y_validate = validate_data['summary_text']\n",
    "max_chapter_length_validate = max(len(chapter.split(' ')) for chapter in x_validate)\n",
    "max_summary_length_validate = max(len(summary.split(' ')) for summary in y_validate)\n",
    "\n",
    "# For test data\n",
    "x_test = test_data['chapter']\n",
    "y_test = test_data['summary_text']\n",
    "max_chapter_length_test = max(len(chapter.split(' ')) for chapter in x_test)\n",
    "max_summary_length_test = max(len(summary.split(' ')) for summary in y_test)\n",
    "\n",
    "# Determine the overall maximum length for each type of data\n",
    "max_length_train = max(max_chapter_length_train, max_summary_length_train) + 1\n",
    "max_length_validate = max(max_chapter_length_validate, max_summary_length_validate) + 1\n",
    "max_length_test = max(max_chapter_length_test, max_summary_length_test) + 1\n",
    "\n",
    "print(\"Training data: Maximum chapter length =\", max_chapter_length_train, \", Maximum summary length =\", max_summary_length_train)\n",
    "print(\"Validation data: Maximum chapter length =\", max_chapter_length_validate, \", Maximum summary length =\", max_summary_length_validate)\n",
    "print(\"Test data: Maximum chapter length =\", max_chapter_length_test, \", Maximum summary length =\", max_summary_length_test)\n",
    "\n",
    "print(\"Overall maximum length for training data:\", max_length_train)\n",
    "print(\"Overall maximum length for validation data:\", max_length_validate)\n",
    "print(\"Overall maximum length for test data:\", max_length_test)\n",
    "\n",
    "MAX_LENGTH = 111989\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bf7af50-aa37-4f14-9e41-e31ad4242119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    ''' Define an encoder in a seq2seq architecture'''\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        ''' Initialize tyhe encoder instance defining its parameters:\n",
    "            Input:\n",
    "                - input_size: the size of the vocabulary\n",
    "                - hidden:size: size of the hidden layer\n",
    "        '''\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # Set the hidden size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Create the embedding layer of size (vocabulary length, hidden_size) \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # Create a GRU layer\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        ''' Run a Forward pass of the encoder to return outputs\n",
    "            Input:\n",
    "                Input: a tensor element (integer) representing the next word in the sentence\n",
    "                hidden: a tensor, the previous hidden state of the encoder\n",
    "        '''\n",
    "        # Get the embedding of the input\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        \n",
    "        # Apply a forward step of the GRU returning the output features and\n",
    "        # the hidden state of the actual time step\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        ''' Initialize the hidden state of the encoder, tensor of zeros'''\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aba278a4-250e-4089-8eeb-3516eec903b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    ''' Define a decoder with atention in a seq2seq architecture'''\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        ''' Initialize the decoder instance defining its parameters:\n",
    "            Input:\n",
    "                - hidden_size:size: size of the hidden layer (Hyperparameter)\n",
    "                - output_size: the size of the vocabulary of the output summary\n",
    "                - dropout_p: dropout probability to apply\n",
    "                - max_length: max length (number of words) of an output or summary\n",
    "        '''\n",
    "\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        # Set parameters of the decoder\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        #Create an embedding layer for the input (output vocabulary, hidden size)\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # Create some linear layers to build the attention mechanism\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        # A dropout layer\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        # A GRU layer\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        # A Fully-connected layer\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        ''' Run a Forward pass of the decoder to return outputs\n",
    "            Input:\n",
    "                Input: a tensor element (integer) representing the previous output of the decoder\n",
    "                hidden: a tensor, the previous hidden state of the decoder\n",
    "                Encoder outputs: a tensor, outputs of the encoder\n",
    "        '''\n",
    "        \n",
    "        #Get the embedding representation of the input\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # Apply dropout \n",
    "        embedded = self.dropout(embedded)\n",
    "        #Calculate the attention weights of the attention mechanism using the encoder states\n",
    "        #in previous time steps\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        \n",
    "        #Calculate the context vectors fo the attention mechanism using the attention weights\n",
    "        # and the encoder outputs\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        # Apply a forward pass to the GRU layer of the decider using the output from the attention\n",
    "        # as input and the hidden state\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        # return the output features, the hidden state and the attention weights\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        ''' Initialize the hidden state of the encoder, tensor of zeros'''\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82a6a6ce-b2e7-4624-9284-ac463a712192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    ''' Transform a sentence in string format to a list of indexes or integers.\n",
    "            The model need to be feeded with numbers, not characters\n",
    "            Input:\n",
    "                - sentence: a string\n",
    "            Output:\n",
    "                - a list of integers, the representation of the sentence in the vector space.\n",
    "    '''\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    ''' Transform a sentence in string format to tensor of indexes or integers.\n",
    "            Out pytorch model work with tensor objects\n",
    "            Input:\n",
    "                - sentence: a string\n",
    "            Output:\n",
    "                - a tensor of integers, the representation of the sentence in the vector space.\n",
    "    '''\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    ''' Convert a pair of text data (source text, summary) to tensors\n",
    "        Input:\n",
    "        - pair: tuple of strings, the source text and its summary\n",
    "        Output:\n",
    "        - tuple of tensors, the input tensor and the outout one\n",
    "    '''\n",
    "    # Convert the source text to the input tensor\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    # Convert the summary to the output tensor\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8bf627e-8dda-478a-881e-db2fd2cb06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16c0f4f5-dc68-44d9-b9db-9419e3566608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    ''' Run all the steps in the training phase of a batch of examples\n",
    "        Input:\n",
    "        - input_tensor: a tensor, vector representation of the input text\n",
    "        - target_tensor: a tensor, vector representation of the expect or labelled output or summary\n",
    "        - encoder: a Class Encoder object, the encoder\n",
    "        - decoder: a Class AttnDecoder object, the decoder\n",
    "        - encoder_optimizer: a torch optimizer, the optimizer of the encoder\n",
    "        - decoer_optimizer: a torch optimizer, the optimizer of the decoder\n",
    "        - criterion: a pytoch loss function\n",
    "        - max_length: an integer, maximun length of an output\n",
    "    '''\n",
    "    #Init the encoder hidden state\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    # Reset the optimizer\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Set the length if the source text and the summary\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    # Create the initial encoder output, all zeros\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    # For every token in the source text or inout\n",
    "    for ei in range(input_length):\n",
    "        # Forward pass of the encoder to get the encoder output and hidden state\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    # Set the initial decoder input as the SOS token\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    #Set the initial decoder hidden state equals to the last encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Active teacher forcing with probability teacher_forcing_ratio \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            # Forward pass of the decoder returning the decoder output, hidden state and context vector\n",
    "            # of the attention mechanism\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Increment the loss function by the loss of the decoder output in the actual time step\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            # Forward pass of the decoder returning the decoder output, hidden state and context vector\n",
    "            # of the attention mechanism\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "             # Select the decoder output with the highest probability\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            # Increment the loss function by the loss of the decoder output in the actual time step\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # Stop training if the EOS token is returned\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "   # Apply the backward pass to calculate and propagate the loss\n",
    "    loss.backward()\n",
    "    # Apply a step of the optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    # Return the final loss\n",
    "    return loss.item() / target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51fbabf3-dad1-4715-922e-3d7c2ad546bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    ''' Return the seconds, s, to a string in the format: Xm Ys'''\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    ''' Return '''\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    ''' Plot the points in a line graph to show a training metric'''\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72733c93-6180-4756-8229-a315e005a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    ''' Train a encoder-decoder model on the input x for n_iters iterations\n",
    "        Input:\n",
    "        - encoder: a Class Encoder object, the encoder\n",
    "        - decoder: a Class AttnDecoder object, the decoder\n",
    "        - x: array of strings, source texts of the training dataset\n",
    "        - y: array of strings, target texts or summaries of the training dataset\n",
    "        - vocab_input: a Vocab Class object, vocabulary of the source texts\n",
    "        - vocab_output: a Vocab Class object, vocabulary of the target texts\n",
    "        - n_iters: integer, number of iterations\n",
    "        - print_every: integer, print the progress every print_every iteration\n",
    "        - plot_every: integer, plot the losses every plot_every iteration\n",
    "        - learning_rate: float, learning rate\n",
    "    '''\n",
    "\n",
    "    print(\"Training....\")\n",
    "    # Get the current time\n",
    "    start = time.time()\n",
    "    # Initialize variables for progress tracking\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    # Create the optimizer for the encoder and the decoder\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    # Extract the training set randomly for all the iterations\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    # Set the function loss to apply\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        if iter% 1000 == 0:\n",
    "            print(iter,\"/\",n_iters + 1) # Plot progress\n",
    "            \n",
    "        # Get the next pair of source text and target to train on\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        # Train on the pair of data selected\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        # Set the variable to plot the progress\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            # Print the ETA and current loss\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            # Plot the current loss\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6cf02f9-64c9-44cd-a42f-64d6aaa001c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, sentence, input_lang, output_lang, max_length=111989):\n",
    "    ''' Function to predict the summary of the source text sentence with a max length\n",
    "        Input:\n",
    "        - encoder: a Class Encoder object, the encoder\n",
    "        - decoder: a Class AttnDecoder object, the decoder\n",
    "        - input_lang: a Vocab Class object, vocabulary of the source texts\n",
    "        - output_lang: a Vocab Class object, vocabulary of the target texts\n",
    "        - sentence: string, source text to predict\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        # Get the tensor of the source text\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        # Calculate the length of the source text\n",
    "        input_length = input_tensor.size()[0]\n",
    "        # Set the initial hidden state of the encoder\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        # Set the initial encoder outputs\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        # For every word in the input\n",
    "        for ei in range(input_length):\n",
    "            # Forward pass of the encoder\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]  # Update encoder_outputs\n",
    "\n",
    "        # Initialize decoder_attentions with the correct dimensions\n",
    "        decoder_attentions = torch.zeros(max_length, input_length)\n",
    "\n",
    "        # Set the initial input of the decoder \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        # Set the initial hidden state of the decoder to the hidden state of the decoder in the last time step\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        # For every word or step in the output sequence\n",
    "        for di in range(max_length):\n",
    "            # Forward pass of the decoder\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Save the decoder attention vector of the step\n",
    "            decoder_attentions[di, :input_length] = decoder_attention.data.squeeze()\n",
    "            # Get the element in the decoder output with the highest probability (the best output)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            # If the token returned is EOS then finish\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                # Append the token in the summary returned by the decoder\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            # Set the decoder input to the output selected\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57d373d2-9a88-4626-88af-55f41ec3d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(x_test, encoder, decoder, input_vocab, output_vocab, max_length=111989, print_every=20):\n",
    "    ''' Generate the predicted summaries of the source texts on x_test\n",
    "        Input:\n",
    "        - x_test: list of strings, the source texts\n",
    "        - encoder: a Class Encoder object, the encoder\n",
    "        - decoder: a Class AttnDecoder object, the decoder\n",
    "        - input_vocab: a Vocab Class object, vocabulary of the source texts\n",
    "        - output_vocab: a Vocab Class object, vocabulary of the target texts\n",
    "        - max_length: integer, max length of the output summary\n",
    "        - print_every: integer, print progress every print_every iterations\n",
    "    '''\n",
    "    predicted_summaries = []\n",
    "    # Set a progress bar\n",
    "    #kbar = pkbar.Kbar(target=len(x_test), width=8)\n",
    "    # Para cada text or document in the validation dataset\n",
    "    for i,doc in enumerate(x_test):\n",
    "        # Predict the summary for the document\n",
    "        #pred_summ = predict(doc,vocab,params,batch_size=1)\n",
    "        pred_summ,_ = predict(encoder, decoder, doc, input_vocab, output_vocab, max_length)\n",
    "        predicted_summaries.append(' '.join(pred_summ[:-1]))\n",
    "        #predicted_summaries.append(' '.join(pred_summ))\n",
    "        \n",
    "        #if i%print_every==0:\n",
    "        #    kbar.update(i)\n",
    "            \n",
    "    # Set teh labeled summaries as the y_test variable, column summary of our dataset\n",
    "    return predicted_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "921aa7e3-676d-4567-a364-901174e516aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = predict(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7e1960b-2e3d-44f7-a4ed-5267993b2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dfd425-0f0a-4934-ac02-2ac3a9b2c7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.2).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82971aea-e5b4-498f-9f7d-21d652efa7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), './enc.w')\n",
    "torch.save(attn_decoder1.state_dict(), './att.w')\n",
    "# Save the vocabularies\n",
    "input_lang.save_to_file('input_vocab.pkl')\n",
    "output_lang.save_to_file('output_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dddf72-d81c-4694-af0c-7d9b5c8b1034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
