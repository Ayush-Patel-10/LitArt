{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1ba532c8-9f17-4d99-a0ef-c6c474dbbfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/patel.ayushj/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f8c2c483-a041-4e12-859e-fb5fa5039e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Training_data.csv')\n",
    "validate_data = pd.read_csv('Validation_data.csv')\n",
    "test_data = pd.read_csv('Testing_data.csv')\n",
    "\n",
    "train_data = train_data[['summary_id','chapter','chapter_length','summary_name','summary_text','summary_analysis','summary_length','analysis_length']]\n",
    "validate_data = validate_data[['summary_id','chapter','chapter_length','summary_name','summary_text','summary_analysis','summary_length','analysis_length']]\n",
    "test_data = test_data[['summary_id','chapter','chapter_length','summary_name','summary_text','summary_analysis','summary_length','analysis_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0de1db1-3e68-4874-9e16-7a3e6adc8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(sentence):\n",
    "    #lower text\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    pattern = r\"\\s*\\([a-zA-Z]\\s_\\)\"\n",
    "    sentence = re.sub(pattern, \"\", sentence)\n",
    "\n",
    "    sentence = sentence.replace(\"\\n\",\" \")\n",
    "\n",
    "    # replacing everything with space\n",
    "    sentence = re.sub(r\"[=.!,¿?.!+,;¿/:|%()<>।॰{}#_'\\\"@$^&*']\", \" \", sentence)\n",
    "    sentence = re.sub(r\"…\", \" \", sentence)\n",
    "\n",
    "    #remove double quotes\n",
    "    sentence = re.sub(r'\"', \" \", sentence)\n",
    "\n",
    "    #remove numbers\n",
    "    sentence = re.sub(r'[0-9]', \"\", sentence)\n",
    "    #sentence = re.sub(r'#([^s]+)', r'1', sentence)\n",
    "\n",
    "    #remove website links\n",
    "    sentence = re.sub('((www.[^s]+)|(https?://[^s]+))','',sentence)\n",
    "\n",
    "    #remove @anythin here\n",
    "    #sentence = re.sub('@[^s]+','',sentence)\n",
    "\n",
    "    #remove multiple spaces\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # remove extra space\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9595b2d0-0a99-41bb-bf95-2d9c58d5c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['summary_text'] = train_data['summary_text'].apply(lambda x: pre_processing(x))\n",
    "train_data['chapter'] = train_data['chapter'].apply(lambda x: pre_processing(x))\n",
    "\n",
    "validate_data['summary_text'] = validate_data['summary_text'].apply(lambda x: pre_processing(x))\n",
    "validate_data['chapter'] = validate_data['chapter'].apply(lambda x: pre_processing(x))\n",
    "\n",
    "test_data['summary_text'] = test_data['summary_text'].apply(lambda x: pre_processing(x))\n",
    "test_data['chapter'] = test_data['chapter'].apply(lambda x: pre_processing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "898c52f5-59da-4f15-b478-ac05354a6e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter_length</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>analysis_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9600.000000</td>\n",
       "      <td>9600.000000</td>\n",
       "      <td>9600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3897.230625</td>\n",
       "      <td>376.896354</td>\n",
       "      <td>274.324063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4203.548176</td>\n",
       "      <td>331.915025</td>\n",
       "      <td>385.446081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1674.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2779.000000</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>133.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4571.000000</td>\n",
       "      <td>467.000000</td>\n",
       "      <td>466.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>114226.000000</td>\n",
       "      <td>4852.000000</td>\n",
       "      <td>5761.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chapter_length  summary_length  analysis_length\n",
       "count     9600.000000     9600.000000      9600.000000\n",
       "mean      3897.230625      376.896354       274.324063\n",
       "std       4203.548176      331.915025       385.446081\n",
       "min         42.000000        2.000000         1.000000\n",
       "25%       1674.000000      171.000000         1.000000\n",
       "50%       2779.000000      283.000000       133.000000\n",
       "75%       4571.000000      467.000000       466.000000\n",
       "max     114226.000000     4852.000000      5761.000000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7e0b6bdb-a764-49bd-9d28-be44b5ce7370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       mine ear is open and my heart prepared the wor...\n",
      "1       before these fields were shorn and tilled full...\n",
      "2       well go thy way thou shalt not from this grove...\n",
      "3       in such a night did thisbe fearfully o ertrip ...\n",
      "4       those strains that once did sweet in zion glid...\n",
      "                              ...                        \n",
      "9595    there was a train for turin and paris that eve...\n",
      "9596    it was not with surprise it was with a feeling...\n",
      "9597    isabel s arrival at gardencourt on this second...\n",
      "9598    he had told her the first evening she ever spe...\n",
      "9599    the life and death of scyld the famous race of...\n",
      "Name: first_60_words, Length: 9600, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to extract first 300 words\n",
    "def extract_first_60_words(text):\n",
    "    words = text.split()[:100]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the function to each chapter\n",
    "train_data['first_60_words'] = train_data['chapter'].apply(extract_first_60_words)\n",
    "validate_data['first_60_words'] = validate_data['chapter'].apply(extract_first_60_words)\n",
    "test_data['first_60_words'] = test_data['chapter'].apply(extract_first_60_words)\n",
    "\n",
    "# Display the result\n",
    "print(train_data['first_60_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d164d3e9-8c0a-4de9-8e8b-ffe1aa504d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       before any characters appear the time and geog...\n",
      "1       in another part of the forest by the river a f...\n",
      "2       when the mounted party from fort howard approa...\n",
      "3       the pursuit of magua is unsuccessful but hawke...\n",
      "4       heyward and the girls are uneasy and gamut is ...\n",
      "                              ...                        \n",
      "9595    before isabel leaves rome she goes to see pans...\n",
      "9596    isabel is greeted by henrietta stackpole at ch...\n",
      "9597    isabel arrives at gardencourt the house is ver...\n",
      "9598    isabel remembers that when she first came to g...\n",
      "9599    beowulf begins with the legends of the warrior...\n",
      "Name: first_60_words_summary, Length: 9600, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Function to extract first 300 words\n",
    "def extract_first_60_words(text):\n",
    "    words = text.split()[:100]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the function to each chapter\n",
    "train_data['first_60_words_summary'] = train_data['summary_text'].apply(extract_first_60_words)\n",
    "validate_data['first_60_words_summary'] = validate_data['summary_text'].apply(extract_first_60_words)\n",
    "test_data['first_60_words_summary'] = test_data['summary_text'].apply(extract_first_60_words)\n",
    "\n",
    "# Display the result\n",
    "print(train_data['first_60_words_summary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a874a4d5-8e43-4f9b-98e7-27f9d00ea0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mine ear is open and my heart prepared the worst is worldly loss thou canst unfold say is my kingdom lost shakespeare it was a feature peculiar to the colonial wars of north america that the toils and dangers of the wilderness were to be encountered before the adverse hosts could meet a wide and apparently an impervious boundary of forests severed the possessions of the hostile provinces of france and england the hardy colonist and the trained european who fought at his side frequently expended months in struggling against the rapids of the streams or in effecting the rugged'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['first_60_words'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f81af08b-92b9-42d4-8fba-a301f22218d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[0:10]\n",
    "validate_data = validate_data[0:10]\n",
    "test_data = test_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f5a9d8ad-ba4e-4589-a707-abe0228a8296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter_length</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>analysis_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5305.400000</td>\n",
       "      <td>356.900000</td>\n",
       "      <td>257.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2350.395626</td>\n",
       "      <td>134.096524</td>\n",
       "      <td>136.337856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3075.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3166.000000</td>\n",
       "      <td>270.250000</td>\n",
       "      <td>150.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4527.500000</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>252.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7428.750000</td>\n",
       "      <td>386.000000</td>\n",
       "      <td>331.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8710.000000</td>\n",
       "      <td>612.000000</td>\n",
       "      <td>473.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       chapter_length  summary_length  analysis_length\n",
       "count       10.000000       10.000000        10.000000\n",
       "mean      5305.400000      356.900000       257.300000\n",
       "std       2350.395626      134.096524       136.337856\n",
       "min       3075.000000      198.000000        75.000000\n",
       "25%       3166.000000      270.250000       150.750000\n",
       "50%       4527.500000      325.000000       252.000000\n",
       "75%       7428.750000      386.000000       331.250000\n",
       "max       8710.000000      612.000000       473.000000"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9ba093b5-cd63-4e47-812b-aa497de4982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        ''' Add every word in a sentence to the vocabulary '''\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        ''' Add a word to the vocabulary'''\n",
    "        if word not in self.word2index:\n",
    "            #Include the word in the mapping from word to index\n",
    "            self.word2index[word] = self.n_words\n",
    "            #Set the count of ocurrencies of the word to 1\n",
    "            self.word2count[word] = 1\n",
    "            # Include the word in the indexes\n",
    "            self.index2word[self.n_words] = word\n",
    "            # Increment by 1 the number of words\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def save_to_file(self, filename):\n",
    "        ''' Save the Vocab object to a file'''\n",
    "        with open(filename,'wb') as f:\n",
    "            pickle.dump(self,f) \n",
    "\n",
    "def load_vocab(filename):\n",
    "    ''' Load a Vocab instance from a file'''\n",
    "    with open(filename,'rb') as f:\n",
    "        v = pickle.load(f)\n",
    "    return v\n",
    "\n",
    "def read_vocabs(text, summary, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Vocab(summary)\n",
    "        output_lang = Vocab(text)\n",
    "    else:\n",
    "        input_lang = Vocab(text)\n",
    "        output_lang = Vocab(summary)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_vocabs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cff25976-e2a4-4c84-b1ae-1040d3804622",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data['first_60_words']\n",
    "y_train = train_data['first_60_words_summary']\n",
    "\n",
    "# x_validate = validate_data['chapter']\n",
    "# y_validate = validate_data['summary_text']\n",
    "\n",
    "# x_test = test_data['chapter']\n",
    "# y_test = test_data['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8ca6a280-a2fb-495f-96b1-d6aa10129c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 10 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "0    mine ear is open and my heart prepared the wor...\n",
      "1    before these fields were shorn and tilled full...\n",
      "2    well go thy way thou shalt not from this grove...\n",
      "3    in such a night did thisbe fearfully o ertrip ...\n",
      "4    those strains that once did sweet in zion glid...\n",
      "5    they do not sleep on yonder cliffs a grisly ba...\n",
      "6    be gay securely dispel my fair with smiles the...\n",
      "7    i fear we shall outsleep the coming morn as mu...\n",
      "8    clo --i am gone sir and anon sir i ll be with ...\n",
      "9    i ll seek a readier path parnell the route tak...\n",
      "Name: first_60_words, dtype: object 545\n",
      "0    before any characters appear the time and geog...\n",
      "1    in another part of the forest by the river a f...\n",
      "2    when the mounted party from fort howard approa...\n",
      "3    the pursuit of magua is unsuccessful but hawke...\n",
      "4    heyward and the girls are uneasy and gamut is ...\n",
      "5    feeling that the cry is some kind of warning w...\n",
      "6    in the stillness that follows heyward finds it...\n",
      "7    though at first menaced by the hurons heyward ...\n",
      "8    since the indians rifles have been placed to t...\n",
      "9    now that the afternoon is shortening hawkeye l...\n",
      "Name: first_60_words_summary, dtype: object 455\n",
      "['be gay securely dispel my fair with smiles the tim rous clouds that hang on thy clear brow death of agrippina the sudden and almost magical change from the stirring incidents of the combat to the stillness that now reigned around him acted on the heated imagination of heyward like some exciting dream while all the images and events he had witnessed remained deeply impressed on his memory he felt a difficulty in persuading himself of their truth still ignorant of the fate of those who had trusted to the aid of the swift current he at first listened intently', 'in the stillness that follows heyward finds it hard to believe what has happened especially as nature seems to reassert itself with the song of birds nonetheless they all hide in the cave gamut still addled and alice trembling and weeping against cora s breast the major closes the inner entrance with the blanket and a pile of sassafras then seats himself with a pistol clenched convulsively in his hand gamut sings isle of wight which is interrupted by savage yells from the center of the island as a rush of voices pours down the island when a triumphant cry']\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabularies of the inout and output data and return the data in pairs of (source text, summary)\n",
    "input_lang, output_lang, pairs = prepare_data( x_train, y_train , False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "97dd7a4c-7b1f-42c1-8ef2-e8dc54789bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: Maximum chapter length = 100 , Maximum summary length = 100\n",
      "Validation data: Maximum chapter length = 100 , Maximum summary length = 100\n",
      "Test data: Maximum chapter length = 100 , Maximum summary length = 100\n",
      "Overall maximum length for training data: 101\n",
      "Overall maximum length for validation data: 101\n",
      "Overall maximum length for test data: 101\n"
     ]
    }
   ],
   "source": [
    "# For training data\n",
    "x_train = train_data['first_60_words']\n",
    "y_train = train_data['first_60_words_summary']\n",
    "max_chapter_length_train = max(len(chapter.split(' ')) for chapter in x_train)\n",
    "max_summary_length_train = max(len(summary.split(' ')) for summary in y_train)\n",
    "\n",
    "# For validation data\n",
    "x_validate = validate_data['first_60_words']\n",
    "y_validate = validate_data['first_60_words_summary']\n",
    "max_chapter_length_validate = max(len(chapter.split(' ')) for chapter in x_validate)\n",
    "max_summary_length_validate = max(len(summary.split(' ')) for summary in y_validate)\n",
    "\n",
    "# For test data\n",
    "x_test = test_data['first_60_words']\n",
    "y_test = test_data['first_60_words_summary']\n",
    "max_chapter_length_test = max(len(chapter.split(' ')) for chapter in x_test)\n",
    "max_summary_length_test = max(len(summary.split(' ')) for summary in y_test)\n",
    "\n",
    "# Determine the overall maximum length for each type of data\n",
    "max_length_train = max(max_chapter_length_train, max_summary_length_train) + 1\n",
    "max_length_validate = max(max_chapter_length_validate, max_summary_length_validate) + 1\n",
    "max_length_test = max(max_chapter_length_test, max_summary_length_test) + 1\n",
    "\n",
    "print(\"Training data: Maximum chapter length =\", max_chapter_length_train, \", Maximum summary length =\", max_summary_length_train)\n",
    "print(\"Validation data: Maximum chapter length =\", max_chapter_length_validate, \", Maximum summary length =\", max_summary_length_validate)\n",
    "print(\"Test data: Maximum chapter length =\", max_chapter_length_test, \", Maximum summary length =\", max_summary_length_test)\n",
    "\n",
    "print(\"Overall maximum length for training data:\", max_length_train)\n",
    "print(\"Overall maximum length for validation data:\", max_length_validate)\n",
    "print(\"Overall maximum length for test data:\", max_length_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "471a64b4-9ae0-4b37-8697-d28e276d99a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 101\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5bf7af50-aa37-4f14-9e41-e31ad4242119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    ''' Define an encoder in a seq2seq architecture'''\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        ''' Initialize tyhe encoder instance defining its parameters:\n",
    "            Input:\n",
    "                - input_size: the size of the vocabulary\n",
    "                - hidden:size: size of the hidden layer\n",
    "        '''\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # Set the hidden size\n",
    "        self.hidden_size = hidden_size\n",
    "        # Create the embedding layer of size (vocabulary length, hidden_size) \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # Create a GRU layer\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        ''' Run a Forward pass of the encoder to return outputs\n",
    "            Input:\n",
    "                Input: a tensor element (integer) representing the next word in the sentence\n",
    "                hidden: a tensor, the previous hidden state of the encoder\n",
    "        '''\n",
    "        # Get the embedding of the input\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        \n",
    "        # Apply a forward step of the GRU returning the output features and\n",
    "        # the hidden state of the actual time step\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        ''' Initialize the hidden state of the encoder, tensor of zeros'''\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aba278a4-250e-4089-8eeb-3516eec903b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    ''' Define a decoder with atention in a seq2seq architecture'''\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        ''' Initialize the decoder instance defining its parameters:\n",
    "            Input:\n",
    "                - hidden_size:size: size of the hidden layer (Hyperparameter)\n",
    "                - output_size: the size of the vocabulary of the output summary\n",
    "                - dropout_p: dropout probability to apply\n",
    "                - max_length: max length (number of words) of an output or summary\n",
    "        '''\n",
    "\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        # Set parameters of the decoder\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        #Create an embedding layer for the input (output vocabulary, hidden size)\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        # Create some linear layers to build the attention mechanism\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        # A dropout layer\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        # A GRU layer\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        # A Fully-connected layer\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        ''' Run a Forward pass of the decoder to return outputs\n",
    "            Input:\n",
    "                Input: a tensor element (integer) representing the previous output of the decoder\n",
    "                hidden: a tensor, the previous hidden state of the decoder\n",
    "                Encoder outputs: a tensor, outputs of the encoder\n",
    "        '''\n",
    "        \n",
    "        #Get the embedding representation of the input\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        # Apply dropout \n",
    "        embedded = self.dropout(embedded)\n",
    "        #Calculate the attention weights of the attention mechanism using the encoder states\n",
    "        #in previous time steps\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        \n",
    "        #Calculate the context vectors fo the attention mechanism using the attention weights\n",
    "        # and the encoder outputs\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        # Apply a forward pass to the GRU layer of the decider using the output from the attention\n",
    "        # as input and the hidden state\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        # return the output features, the hidden state and the attention weights\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        ''' Initialize the hidden state of the encoder, tensor of zeros'''\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "82a6a6ce-b2e7-4624-9284-ac463a712192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    ''' Transform a sentence in string format to a list of indexes or integers.\n",
    "            The model need to be feeded with numbers, not characters\n",
    "            Input:\n",
    "                - sentence: a string\n",
    "            Output:\n",
    "                - a list of integers, the representation of the sentence in the vector space.\n",
    "    '''\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    ''' Transform a sentence in string format to tensor of indexes or integers.\n",
    "            Out pytorch model work with tensor objects\n",
    "            Input:\n",
    "                - sentence: a string\n",
    "            Output:\n",
    "                - a tensor of integers, the representation of the sentence in the vector space.\n",
    "    '''\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    ''' Convert a pair of text data (source text, summary) to tensors\n",
    "        Input:\n",
    "        - pair: tuple of strings, the source text and its summary\n",
    "        Output:\n",
    "        - tuple of tensors, the input tensor and the outout one\n",
    "    '''\n",
    "    # Convert the source text to the input tensor\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    # Convert the summary to the output tensor\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d8bf627e-8dda-478a-881e-db2fd2cb06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "16c0f4f5-dc68-44d9-b9db-9419e3566608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    ''' Run all the steps in the training phase of a batch of examples\n",
    "        Input:\n",
    "        - input_tensor: a tensor, vector representation of the input text\n",
    "        - target_tensor: a tensor, vector representation of the expect or labelled output or summary\n",
    "        - encoder: a Class Encoder object, the encoder\n",
    "        - decoder: a Class AttnDecoder object, the decoder\n",
    "        - encoder_optimizer: a torch optimizer, the optimizer of the encoder\n",
    "        - decoer_optimizer: a torch optimizer, the optimizer of the decoder\n",
    "        - criterion: a pytoch loss function\n",
    "        - max_length: an integer, maximun length of an output\n",
    "    '''\n",
    "    #Init the encoder hidden state\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    # Reset the optimizer\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Set the length if the source text and the summary\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    # Create the initial encoder output, all zeros\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    # For every token in the source text or inout\n",
    "    for ei in range(input_length):\n",
    "        # Forward pass of the encoder to get the encoder output and hidden state\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "        \n",
    "    # Set the initial decoder input as the SOS token\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    #Set the initial decoder hidden state equals to the last encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Active teacher forcing with probability teacher_forcing_ratio \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            # Forward pass of the decoder returning the decoder output, hidden state and context vector\n",
    "            # of the attention mechanism\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Increment the loss function by the loss of the decoder output in the actual time step\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            # Forward pass of the decoder returning the decoder output, hidden state and context vector\n",
    "            # of the attention mechanism\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "             # Select the decoder output with the highest probability\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            # Increment the loss function by the loss of the decoder output in the actual time step\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            # Stop training if the EOS token is returned\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "   # Apply the backward pass to calculate and propagate the loss\n",
    "    loss.backward()\n",
    "    # Apply a step of the optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    # Return the final loss\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "51fbabf3-dad1-4715-922e-3d7c2ad546bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    ''' Return the seconds, s, to a string in the format: Xm Ys'''\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    ''' Return '''\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    ''' Plot the points in a line graph to show a training metric'''\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "72733c93-6180-4756-8229-a315e005a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    ''' Train a encoder-decoder model on the input x for n_iters iterations\n",
    "        Input:\n",
    "        - encoder: a Class Encoder object, the encoder\n",
    "        - decoder: a Class AttnDecoder object, the decoder\n",
    "        - x: array of strings, source texts of the training dataset\n",
    "        - y: array of strings, target texts or summaries of the training dataset\n",
    "        - vocab_input: a Vocab Class object, vocabulary of the source texts\n",
    "        - vocab_output: a Vocab Class object, vocabulary of the target texts\n",
    "        - n_iters: integer, number of iterations\n",
    "        - print_every: integer, print the progress every print_every iteration\n",
    "        - plot_every: integer, plot the losses every plot_every iteration\n",
    "        - learning_rate: float, learning rate\n",
    "    '''\n",
    "\n",
    "    print(\"Training....\")\n",
    "    # Get the current time\n",
    "    start = time.time()\n",
    "    # Initialize variables for progress tracking\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "    # Create the optimizer for the encoder and the decoder\n",
    "    encoder_optimizer = optim.AdamW(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "    # Extract the training set randomly for all the iterations\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    # Set the function loss to apply\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        if iter% 1000 == 0:\n",
    "            print(iter,\"/\",n_iters + 1) # Plot progress\n",
    "            \n",
    "        # Get the next pair of source text and target to train on\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        # Train on the pair of data selected\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        # Set the variable to plot the progress\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            # Print the ETA and current loss\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            # Plot the current loss\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b6cf02f9-64c9-44cd-a42f-64d6aaa001c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(encoder, decoder, sentence, input_lang, output_lang, max_length=MAX_LENGTH):\n",
    "    ''' Function to predict the summary of the source text sentence with a max length\n",
    "        Input:\n",
    "        - encoder: a Class Encoder object, the encoder\n",
    "        - decoder: a Class AttnDecoder object, the decoder\n",
    "        - input_lang: a Vocab Class object, vocabulary of the source texts\n",
    "        - output_lang: a Vocab Class object, vocabulary of the target texts\n",
    "        - sentence: string, source text to predict\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        # Get the tensor of the source text\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        # Calculate the length of the source text\n",
    "        input_length = input_tensor.size()[0]\n",
    "        # Set the initial hidden state of the encoder\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        # Set the initial encoder outputs\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        # For every word in the input\n",
    "        for ei in range(input_length):\n",
    "            # Forward pass of the encoder\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]  # Update encoder_outputs\n",
    "\n",
    "        # Initialize decoder_attentions with the correct dimensions\n",
    "        decoder_attentions = torch.zeros(max_length, input_length)\n",
    "\n",
    "        # Set the initial input of the decoder \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        # Set the initial hidden state of the decoder to the hidden state of the decoder in the last time step\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        # For every word or step in the output sequence\n",
    "        for di in range(max_length):\n",
    "            # Forward pass of the decoder\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Save the decoder attention vector of the step\n",
    "            decoder_attentions[di, :input_length] = decoder_attention.data.squeeze()\n",
    "            # Get the element in the decoder output with the highest probability (the best output)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            # If the token returned is EOS then finish\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                # Append the token in the summary returned by the decoder\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            # Set the decoder input to the output selected\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "57d373d2-9a88-4626-88af-55f41ec3d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(x_test, encoder, decoder, input_vocab, output_vocab, max_length, print_every=20):\n",
    "    ''' Generate the predicted summaries of the source texts on x_test\n",
    "        Input:\n",
    "        - x_test: list of strings, the source texts\n",
    "        - encoder: a Class Encoder object, the encoder\n",
    "        - decoder: a Class AttnDecoder object, the decoder\n",
    "        - input_vocab: a Vocab Class object, vocabulary of the source texts\n",
    "        - output_vocab: a Vocab Class object, vocabulary of the target texts\n",
    "        - max_length: integer, max length of the output summary\n",
    "        - print_every: integer, print progress every print_every iterations\n",
    "    '''\n",
    "    predicted_summaries = []\n",
    "    # Set a progress bar\n",
    "    #kbar = pkbar.Kbar(target=len(x_test), width=8)\n",
    "    # Para cada text or document in the validation dataset\n",
    "    for i,doc in enumerate(x_test):\n",
    "        # Predict the summary for the document\n",
    "        #pred_summ = predict(doc,vocab,params,batch_size=1)\n",
    "        pred_summ,_ = predict(encoder, decoder, doc, input_vocab, output_vocab, max_length)\n",
    "        predicted_summaries.append(' '.join(pred_summ[:-1]))\n",
    "        #predicted_summaries.append(' '.join(pred_summ))\n",
    "        \n",
    "        #if i%print_every==0:\n",
    "        #    kbar.update(i)\n",
    "            \n",
    "    # Set teh labeled summaries as the y_test variable, column summary of our dataset\n",
    "    return predicted_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "921aa7e3-676d-4567-a364-901174e516aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = predict(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a7e1960b-2e3d-44f7-a4ed-5267993b2378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "46dfd425-0f0a-4934-ac02-2ac3a9b2c7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n",
      "0m 16s (- 13m 9s) (100 2%) 5.1421\n",
      "0m 32s (- 12m 48s) (200 4%) 4.2405\n",
      "0m 47s (- 12m 29s) (300 6%) 4.0311\n",
      "1m 3s (- 12m 12s) (400 8%) 3.9682\n",
      "1m 19s (- 11m 55s) (500 10%) 3.8542\n",
      "1m 34s (- 11m 36s) (600 12%) 3.6254\n",
      "1m 50s (- 11m 20s) (700 14%) 3.9851\n",
      "2m 6s (- 11m 1s) (800 16%) 3.1434\n",
      "2m 21s (- 10m 44s) (900 18%) 3.7077\n",
      "1000 / 5001\n",
      "2m 37s (- 10m 28s) (1000 20%) 3.7631\n",
      "2m 52s (- 10m 11s) (1100 22%) 3.4893\n",
      "3m 7s (- 9m 54s) (1200 24%) 2.9858\n",
      "3m 23s (- 9m 38s) (1300 26%) 3.1708\n",
      "3m 38s (- 9m 22s) (1400 28%) 3.2616\n",
      "3m 54s (- 9m 6s) (1500 30%) 3.5877\n",
      "4m 9s (- 8m 50s) (1600 32%) 3.7465\n",
      "4m 25s (- 8m 34s) (1700 34%) 3.4385\n",
      "4m 40s (- 8m 18s) (1800 36%) 2.9138\n",
      "4m 56s (- 8m 3s) (1900 38%) 3.8292\n",
      "2000 / 5001\n",
      "5m 11s (- 7m 47s) (2000 40%) 3.4716\n",
      "5m 26s (- 7m 30s) (2100 42%) 3.6602\n",
      "5m 42s (- 7m 15s) (2200 44%) 3.8371\n",
      "5m 57s (- 6m 59s) (2300 46%) 3.6999\n",
      "6m 12s (- 6m 43s) (2400 48%) 3.9038\n",
      "6m 28s (- 6m 28s) (2500 50%) 3.3541\n",
      "6m 43s (- 6m 12s) (2600 52%) 2.9057\n",
      "6m 58s (- 5m 56s) (2700 54%) 3.3697\n",
      "7m 14s (- 5m 41s) (2800 56%) 3.0812\n",
      "7m 30s (- 5m 25s) (2900 57%) 3.4232\n",
      "3000 / 5001\n",
      "7m 45s (- 5m 10s) (3000 60%) 3.4258\n",
      "8m 0s (- 4m 54s) (3100 62%) 2.8345\n",
      "8m 16s (- 4m 39s) (3200 64%) 2.7807\n",
      "8m 31s (- 4m 23s) (3300 66%) 3.0903\n",
      "8m 47s (- 4m 8s) (3400 68%) 3.2431\n",
      "9m 2s (- 3m 52s) (3500 70%) 3.1430\n",
      "9m 17s (- 3m 36s) (3600 72%) 3.0101\n",
      "9m 33s (- 3m 21s) (3700 74%) 3.0339\n",
      "9m 48s (- 3m 5s) (3800 76%) 2.6275\n",
      "10m 3s (- 2m 50s) (3900 78%) 3.0681\n",
      "4000 / 5001\n",
      "10m 19s (- 2m 34s) (4000 80%) 3.0808\n",
      "10m 34s (- 2m 19s) (4100 82%) 3.3414\n",
      "10m 50s (- 2m 3s) (4200 84%) 3.6065\n",
      "11m 5s (- 1m 48s) (4300 86%) 3.4247\n",
      "11m 21s (- 1m 32s) (4400 88%) 3.2206\n",
      "11m 35s (- 1m 17s) (4500 90%) 2.9775\n",
      "11m 51s (- 1m 1s) (4600 92%) 3.0273\n",
      "12m 6s (- 0m 46s) (4700 94%) 2.7353\n",
      "12m 21s (- 0m 30s) (4800 96%) 2.7064\n",
      "12m 36s (- 0m 15s) (4900 98%) 2.5645\n",
      "5000 / 5001\n",
      "12m 51s (- 0m 0s) (5000 100%) 2.9716\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.2).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 5000, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "82971aea-e5b4-498f-9f7d-21d652efa7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1.state_dict(), './enc.w')\n",
    "torch.save(attn_decoder1.state_dict(), './att.w')\n",
    "# Save the vocabularies\n",
    "input_lang.save_to_file('input_vocab.pkl')\n",
    "output_lang.save_to_file('output_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "476ad393-f0e7-4048-a8dc-80f59ed13f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "torch.save(encoder1.state_dict(), 'encoder_checkpoint.pth')\n",
    "torch.save(attn_decoder1.state_dict(), 'decoder_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "54dddf72-d81c-4694-af0c-7d9b5c8b1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = validate_data['first_60_words'].values\n",
    "y_test = validate_data['first_60_words_summary'].values\n",
    "# Generate the predctions on the validation dataset\n",
    "predicted_summaries = generate_predictions(x_test, encoder1, attn_decoder1, input_lang, output_lang, MAX_LENGTH, 100)\n",
    "# Set teh labeled summaries as the y_test variable, column summary of our dataset\n",
    "labeled_summaries = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "005a3273-d319-414b-b417-838fedb5a906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Pred:  in another part of the forest to the river is now of his tribe of the forest habits of his tribe of the forest by a long semi-nude war-painted body and chingachgook with his hunting river and chingachgook with his hunting shirt to be his semi-nude war-painted body the forest hawkeye and chingachgook to be forest by the river and chingachgook to be \n",
      " Target:  in another part of the forest by the river a few miles to the west hawkeye and chingachgook appear to be waiting for someone as they talk with low voices it is now afternoon the indian and the scout are attired according to their forest habits chingachgook with his semi-nude war-painted body and scalping tuft of hair his tomahawk scalping knife and short rifle hawkeye with his hunting shirt skin cap buckskin leggings knife pouch and horn and long rifle they discuss their respective forefathers and chingachgook relates the slow demise of his tribe of mohicans so that only he\n",
      "\n",
      " Pred:  when the mounted party from fort howard approaches the three men of the woods hawkeye addresses the gamut the the the william henry doubtful especially when the gamut then heyward only to learn that the gamut of then heyward only to learn that the gamut and then heyward only to learn that the gamut and then heyward only to learn that the gamut then heyward only to learn the gamut and then heyward only to learn that the gamut then heyward only to learn that the gamut the the william toward then henry doubtful especially when west instead of the \n",
      " Target:  when the mounted party from fort howard approaches the three men of the woods hawkeye addresses first gamut and then heyward only to learn that they are lost because their indian guide has taken them west instead of north toward fort william henry doubtful especially when he learns that the guide is a huron who has been adopted by the mohawks hawkeye makes an a priori judgment of the still-unseen guide and uses the contemptuous term mingo he who is born a mingo will die a mingo his two indian companions concur with his thinking still doubting and cautious be\n",
      "\n",
      " Pred:  the pursuit of is unsuccessful has wounded him slightly and certain of find their which upon them the to the to the to the the the to the to the to the to the the to the to the the the to the the to the the to the the to the the to the to the to the to the to the the to the the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to the to \n",
      " Target:  the pursuit of magua is unsuccessful but hawkeye feels that he has wounded him slightly and is certain of it when they find bloodstains on the sumach leaves heyward wants to continue the chase but the scout fears an ambush particularly since he has fired his rifle an action for which he upbraids himself with night almost upon them the three woodsmen confer and at the urging of uncas decide to take the group to their harboring place after heyward promises to keep the place a secret the horses are a problem but rather than give them their bridles the\n",
      "\n",
      " Pred:  heyward and the girls have gamut have uneasy and gamut have gamut and is in spirit light flashes a light flashes upon pine of pine have entered have entered have entered silhouettes relieves knot knot knot knot of pine which silhouettes uncas the others have entered have entered a blazing knot knot of pine which silhouettes uncas the others have silhouettes by the blanket hawkeye is holding a blazing knot knot knot knot \n",
      " Target:  heyward and the girls are uneasy and gamut is still struggling in spirit when a light flashes upon them and they see that the others have entered a cavern hidden by a blanket hawkeye is holding a blazing knot of pine which silhouettes uncas the first clear sight of whose carriage and almost grecian features relieves the lingering doubts of those from fort edward when the latter also enter the cavern they learn that at the other entrance is a narrow open chasm running at right angles and that just beyond it is another cave they are essentially on an\n",
      "\n",
      " Pred:  feeling that the cry is some kind of warning whether intended or not hawkeye leads the entire party from the caves as heyward now recognizes the horrifying sound comes again as the shriek recognizes shriek of the horrifying sound comes again as the shriek loveliness of his in his knowledge of things \n",
      " Target:  feeling that the cry is some kind of warning whether intended or not hawkeye leads the entire party from the caves as heyward remarks upon the loveliness of the scene the horrifying sound comes again as if from the bed of the river and heyward now recognizes it as the shriek of a horse in terror the scout s reckoning that the horses are frightened by hovering wolves is immediately confirmed by a long howl that swiftly recedes into the depths of the forest an abandonment that indicates enemy indians are near once again secure in his knowledge of things\n"
     ]
    }
   ],
   "source": [
    "print('\\n Pred: ',predicted_summaries[1],'\\n Target: ', labeled_summaries[1])\n",
    "print('\\n Pred: ',predicted_summaries[2],'\\n Target: ', labeled_summaries[2])\n",
    "print('\\n Pred: ',predicted_summaries[3],'\\n Target: ', labeled_summaries[3])\n",
    "print('\\n Pred: ',predicted_summaries[4],'\\n Target: ', labeled_summaries[4])\n",
    "print('\\n Pred: ',predicted_summaries[5],'\\n Target: ', labeled_summaries[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e214f8af-1fb7-4f53-b961-d2c540ce4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_textfile(filename, strings):\n",
    "    ''' Save the contect of a list of strings to a file called filename\n",
    "    \n",
    "        Input:\n",
    "           - filename: name of the file to save the strings\n",
    "           - strings: a list of string to save to disk\n",
    "    '''\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        for item in strings:\n",
    "            #Remove any \\n in the string\n",
    "            item = remove_CTL(item)\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "def eval_metrics(preds, targets, avg=True):\n",
    "    ''' Evaluate the ROUGE metrics ROUGE-2 and ROUGE-L for every pair predicted summary - target summary\n",
    "    \n",
    "        Input:\n",
    "           - preds: list of strings, predicted summaries\n",
    "           - targets: list of string, target summaries\n",
    "        Output:\n",
    "            - rouge2_f_metric: list of float, the Rouge-2 fscore for every predicted summary\n",
    "            - rougel_f_metric: list of float, the Rouge-L fscore for every predicted summary\n",
    "    '''\n",
    "    #Lets calculate the rouge metrics for every document\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(preds, targets, avg)\n",
    "    # Create the output variables\n",
    "    if avg:\n",
    "        rouge2_f_metric = scores['rouge-2']['f']\n",
    "        rouge2_p_metric = scores['rouge-2']['p']\n",
    "        rouge2_r_metric = scores['rouge-2']['r']\n",
    "        rougel_f_metric = scores['rouge-l']['f']\n",
    "        rougel_p_metric = scores['rouge-l']['p']\n",
    "        rougel_r_metric = scores['rouge-l']['r']\n",
    "    else:\n",
    "        rouge2_f_metric = [score['rouge-2']['f'] for score in scores]\n",
    "        rouge2_p_metric = [score['rouge-2']['p'] for score in scores]\n",
    "        rouge2_r_metric = [score['rouge-2']['r'] for score in scores]\n",
    "        rougel_f_metric = [score['rouge-l']['f'] for score in scores]\n",
    "        rougel_p_metric = [score['rouge-l']['p'] for score in scores]\n",
    "        rougel_r_metric = [score['rouge-l']['r'] for score in scores]\n",
    "\n",
    "    return rouge2_f_metric, rouge2_p_metric, rouge2_r_metric, rougel_f_metric, rougel_p_metric, rougel_r_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "82db3760-231e-4b4a-9eda-6f31e165b925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Rouge-2 FScore:  0.2669670780243491 Mean Rouge-L FScore:  0.4223856568250451\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Rouge-2 and Rouge-L metrics for the validation dataset\n",
    "r2_f, r2_p, r2_r, rl_f, rl_p, rl_r = eval_metrics(predicted_summaries, list(labeled_summaries), False)\n",
    "print('Mean Rouge-2 FScore: ',np.mean(r2_f), 'Mean Rouge-L FScore: ',np.mean(rl_f))\n",
    "#Store the results on the dataframe\n",
    "validate_data['pred_summary'] = predicted_summaries\n",
    "validate_data['rouge2-f'] = r2_f\n",
    "validate_data['rouge2-p'] = r2_p\n",
    "validate_data['rouge2-r'] = r2_r\n",
    "validate_data['rougel-f'] = rl_f\n",
    "validate_data['rougel-p'] = rl_p\n",
    "validate_data['rougel-r'] = rl_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ad7f532d-59dd-4b18-9442-6053e2a844b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>chapter</th>\n",
       "      <th>chapter_length</th>\n",
       "      <th>summary_name</th>\n",
       "      <th>summary_text</th>\n",
       "      <th>summary_analysis</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>analysis_length</th>\n",
       "      <th>first_60_words</th>\n",
       "      <th>first_60_words_summary</th>\n",
       "      <th>pred_summary</th>\n",
       "      <th>rouge2-f</th>\n",
       "      <th>rouge2-p</th>\n",
       "      <th>rouge2-r</th>\n",
       "      <th>rougel-f</th>\n",
       "      <th>rougel-p</th>\n",
       "      <th>rougel-r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chapters 1-2</td>\n",
       "      <td>mine ear is open and my heart prepared the wor...</td>\n",
       "      <td>6471.0</td>\n",
       "      <td>Chapters 1-2</td>\n",
       "      <td>before any characters appear the time and geog...</td>\n",
       "      <td>These two chapters introduce the reader to the...</td>\n",
       "      <td>388.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>mine ear is open and my heart prepared the wor...</td>\n",
       "      <td>before any characters appear the time and geog...</td>\n",
       "      <td>in the time and is the last war to the horrors...</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.162162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chapter 3</td>\n",
       "      <td>before these fields were shorn and tilled full...</td>\n",
       "      <td>3132.0</td>\n",
       "      <td>Chapter 3</td>\n",
       "      <td>in another part of the forest by the river a f...</td>\n",
       "      <td>This chapter introduces the other three main a...</td>\n",
       "      <td>198.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>before these fields were shorn and tilled full...</td>\n",
       "      <td>in another part of the forest by the river a f...</td>\n",
       "      <td>in another part of the forest to the river is ...</td>\n",
       "      <td>0.335404</td>\n",
       "      <td>0.435484</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.271429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chapter 4</td>\n",
       "      <td>well go thy way thou shalt not from this grove...</td>\n",
       "      <td>3075.0</td>\n",
       "      <td>Chapter 4</td>\n",
       "      <td>when the mounted party from fort howard approa...</td>\n",
       "      <td>Since this chapter is mostly one of surface ac...</td>\n",
       "      <td>319.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>well go thy way thou shalt not from this grove...</td>\n",
       "      <td>when the mounted party from fort howard approa...</td>\n",
       "      <td>when the mounted party from fort howard approa...</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.342466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chapter 5</td>\n",
       "      <td>in such a night did thisbe fearfully o ertrip ...</td>\n",
       "      <td>3268.0</td>\n",
       "      <td>Chapter 5</td>\n",
       "      <td>the pursuit of magua is unsuccessful but hawke...</td>\n",
       "      <td>Here the reader encounters the first bloodshed...</td>\n",
       "      <td>329.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>in such a night did thisbe fearfully o ertrip ...</td>\n",
       "      <td>the pursuit of magua is unsuccessful but hawke...</td>\n",
       "      <td>the pursuit of is unsuccessful has wounded him...</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.178082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chapter 6</td>\n",
       "      <td>those strains that once did sweet in zion glid...</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>Chapter 6</td>\n",
       "      <td>heyward and the girls are uneasy and gamut is ...</td>\n",
       "      <td>This chapter shows Cooper in his most inventiv...</td>\n",
       "      <td>321.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>those strains that once did sweet in zion glid...</td>\n",
       "      <td>heyward and the girls are uneasy and gamut is ...</td>\n",
       "      <td>heyward and the girls have gamut have uneasy a...</td>\n",
       "      <td>0.292398</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.315068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     summary_id                                            chapter  \\\n",
       "0  chapters 1-2  mine ear is open and my heart prepared the wor...   \n",
       "1     chapter 3  before these fields were shorn and tilled full...   \n",
       "2     chapter 4  well go thy way thou shalt not from this grove...   \n",
       "3     chapter 5  in such a night did thisbe fearfully o ertrip ...   \n",
       "4     chapter 6  those strains that once did sweet in zion glid...   \n",
       "\n",
       "   chapter_length  summary_name  \\\n",
       "0          6471.0  Chapters 1-2   \n",
       "1          3132.0     Chapter 3   \n",
       "2          3075.0     Chapter 4   \n",
       "3          3268.0     Chapter 5   \n",
       "4          3873.0     Chapter 6   \n",
       "\n",
       "                                        summary_text  \\\n",
       "0  before any characters appear the time and geog...   \n",
       "1  in another part of the forest by the river a f...   \n",
       "2  when the mounted party from fort howard approa...   \n",
       "3  the pursuit of magua is unsuccessful but hawke...   \n",
       "4  heyward and the girls are uneasy and gamut is ...   \n",
       "\n",
       "                                    summary_analysis  summary_length  \\\n",
       "0  These two chapters introduce the reader to the...           388.0   \n",
       "1  This chapter introduces the other three main a...           198.0   \n",
       "2  Since this chapter is mostly one of surface ac...           319.0   \n",
       "3  Here the reader encounters the first bloodshed...           329.0   \n",
       "4  This chapter shows Cooper in his most inventiv...           321.0   \n",
       "\n",
       "   analysis_length                                     first_60_words  \\\n",
       "0            473.0  mine ear is open and my heart prepared the wor...   \n",
       "1            149.0  before these fields were shorn and tilled full...   \n",
       "2             75.0  well go thy way thou shalt not from this grove...   \n",
       "3            156.0  in such a night did thisbe fearfully o ertrip ...   \n",
       "4            128.0  those strains that once did sweet in zion glid...   \n",
       "\n",
       "                              first_60_words_summary  \\\n",
       "0  before any characters appear the time and geog...   \n",
       "1  in another part of the forest by the river a f...   \n",
       "2  when the mounted party from fort howard approa...   \n",
       "3  the pursuit of magua is unsuccessful but hawke...   \n",
       "4  heyward and the girls are uneasy and gamut is ...   \n",
       "\n",
       "                                        pred_summary  rouge2-f  rouge2-p  \\\n",
       "0  in the time and is the last war to the horrors...  0.151515  0.151515   \n",
       "1  in another part of the forest to the river is ...  0.335404  0.435484   \n",
       "2  when the mounted party from fort howard approa...  0.303030  0.303030   \n",
       "3  the pursuit of is unsuccessful has wounded him...  0.101010  0.101010   \n",
       "4  heyward and the girls have gamut have uneasy a...  0.292398  0.347222   \n",
       "\n",
       "   rouge2-r  rougel-f  rougel-p  rougel-r  \n",
       "0  0.151515  0.263736  0.705882  0.162162  \n",
       "1  0.272727  0.395833  0.730769  0.271429  \n",
       "2  0.303030  0.490196  0.862069  0.342466  \n",
       "3  0.101010  0.288889  0.764706  0.178082  \n",
       "4  0.252525  0.455446  0.821429  0.315068  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_data.to_csv('results.csv', index=False)\n",
    "validate_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4287867-cda7-42af-8b75-fc8376f98062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
