{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 11:36:03.505718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 11:36:05.257944: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from datetime import date\n",
    "\n",
    "import torch\n",
    "#import lightning as L\n",
    "#from lightning.pytorch.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "#Transformers\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from transformers import AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "#from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "#Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "#PEFT\n",
    "from peft import LoraConfig\n",
    "from peft import PeftConfig\n",
    "from peft import PeftModel\n",
    "from peft import get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to print the number of trainable parameters in the given model\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable %: {100 * trainable_params / all_param}\")\n",
    "\n",
    "def tokenize_input(df,tokenizer,tokenizer_chapter_max_length,tokenizer_summary_max_length):\n",
    "\n",
    "    prompt_start = \"Summarize the following : \\n\"\n",
    "    prompt_end = \"\\n\\nSummary:\"\n",
    "\n",
    "    prompt = [prompt_start + dialogue + prompt_end for dialogue in df[\"chapter\"]]\n",
    "\n",
    "    df[\"input_ids\"] = tokenizer(prompt, max_length=tokenizer_chapter_max_length , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "    df[\"labels\"] = tokenizer(df[\"summary_text\"],max_length=tokenizer_summary_max_length , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/work/LitArt/nair/cache/\" \n",
    "log_path = \"/work/LitArt/nair/outdir/\"\n",
    "\n",
    "tokenizer_chapter_max_length = 1024\n",
    "tokenizer_summary_max_length = 256\n",
    "model = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "#Training Parameters\n",
    "batch_size = 2\n",
    "epochs = 1\n",
    "log_path = log_path+model.replace(\"/\",\"-\")+\"-\" +str(today)+\"-\"+time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "#logger = TensorBoardLogger(log_path, name=\"my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813cc6fc035a4fabaefc77dbdec49a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<PAD>\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Enable gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce the memory consumption during the backward pas. Instead of storing all intermediate activations in the forward pass (which is what's typically done to compute gradients in the backward pass), gradient checkpointing stores only a subset of them\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mbase_model\u001b[49m\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable() \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Prepare the model for k-bit training . Applies some preprocessing to the model to prepare it for training.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m base_model \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(base_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "cache_dir = \"/work/LitArt/cache\" \n",
    "\n",
    "#Bits and Bytes config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "load_in_4bit=True, #4bit quantizaition - load_in_4bit is used to load models in 4-bit quantization \n",
    "bnb_4bit_use_double_quant=True, #nested quantization technique for even greater memory efficiency without sacrificing performance. This technique has proven beneficial, especially when fine-tuning large models\n",
    "bnb_4bit_quant_type=\"nf4\", #quantization type used is 4 bit Normal Float Quantization- The NF4 data type is designed for weights initialized using a normal distribution\n",
    "bnb_4bit_compute_dtype=torch.bfloat16, #modify the data type used during computation. This can result in speed improvements. \n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model,\n",
    "                                                    device_map=\"auto\",\n",
    "                                                    trust_remote_code=True,\n",
    "                                                    quantization_config=bnb_config,\n",
    "                                                    cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "\n",
    "# Enable gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce the memory consumption during the backward pas. Instead of storing all intermediate activations in the forward pass (which is what's typically done to compute gradients in the backward pass), gradient checkpointing stores only a subset of them\n",
    "base_model.gradient_checkpointing_enable() \n",
    "\n",
    "# Prepare the model for k-bit training . Applies some preprocessing to the model to prepare it for training.\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \n",
    "                    \"o_proj\", \n",
    "                    \"k_proj\", \n",
    "                    \"v_proj\", \n",
    "                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config)\n",
    "\n",
    "#base_model = get_peft_model(base_model, config)\n",
    "\n",
    "# Print the number of trainable parameters in the model\n",
    "print_trainable_parameters(base_model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('csv', \n",
    "                    data_files={\n",
    "                        'train': \"/work/LitArt/data/chunked_dataset/train_dataset_with_summaries.csv\",\n",
    "                        'test': \"/work/LitArt/data/chunked_dataset/test_dataset_with_summaries.csv\",\n",
    "                        'val':\"/work/LitArt/data/chunked_dataset/validation_dataset_with_summaries.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = data[\"train\"].shuffle().map(tokenize_input, batched=True, fn_kwargs={\"tokenizer\": tokenizer, \"tokenizer_chapter_max_length\": tokenizer_chapter_max_length,\"tokenizer_summary_max_length\":tokenizer_summary_max_length})\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['chapter', 'human_summary', '__index_level_0__', 'summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "output_dir = f\"llama-7b-qlora-Capstone-project\"\n",
    "per_device_train_batch_size = batch_size\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\" #\"paged_adamw_8bit\"\n",
    "save_steps = 10\n",
    "save_total_limit=3\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps =20 #1000\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\" #\"cosine\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=log_path,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    #save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    #save_strategy='epoch',\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_checkpointing=True,\n",
    "    #push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### USER: {example['chapter']}\\n### ASSISTANT: {example['summary_text']}\"\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=data[\"train\"],\n",
    "    packing=True,\n",
    "    #dataset_text_field=\"id\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=1024,\n",
    "    formatting_func=formatting_func,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 08:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.650800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.623700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=2.637235641479492, metrics={'train_runtime': 509.5598, 'train_samples_per_second': 0.314, 'train_steps_per_second': 0.039, 'total_flos': 6514932543651840.0, 'train_loss': 2.637235641479492, 'epoch': 0.09})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/LitArt/verma/tiiuae-falcon-7b-2024-03-18-20:27:30'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d012eda4cb4100abb87de840c341d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=quantization_config,\n",
    "    #adapter_kwargs={\"revision\": \"09487e6ffdcc75838b10b6138b6149c36183164e\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### USER:' text to be summarised' ### Assistant: the text is a summary of the events that occurred in the city of bethlehem on the night of december 24th 1999 the summary begins with the arrival of a group of strangers in the city led by a man named john who claims to be the messiah the strangers are welcomed by the townspeople but are soon met with resistance from the town s leaders the strangers are forced to leave the city but not before they have left a trail of destruction in their wake the summary ends with the strangers continuing their journey towards their destination the summary is a powerful and thought-provoking piece of literature that leaves the reader with many unanswered questions and a sense of unease</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def generate_response(chapter : str) -> str:\n",
    "    prompt =  f\"\"\"### USER:' {chapter}' ### Assistant:  \"\"\".strip()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
    "    outputs = model.generate(inputs.input_ids, max_new_tokens=250, do_sample=False)\n",
    "    return(tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "\n",
    "\n",
    "'''\n",
    "\tencoding = tokenizer(prompt, return_tensors = \"pt\").to(DEVICE)\n",
    "\t#with torch.inference_mode():\n",
    "    with torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\tinput_ids=encoding.input_ids,\n",
    "\t\t\tattention_mask=encoding.attention_mask,\n",
    "\t\t\tgeneration_config=generation_config,\n",
    "\t\t)\n",
    "\n",
    "\tresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t#assistant_start =  \"<assistant>:\"\n",
    "\t#response_start = response.find(assistant_start)\n",
    "\t#return response[response_start + len(assistant_start) : ].strip()\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "chapter = \"text to be summarised\"\n",
    "print (generate_response(chapter))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration for the trained model\n",
    "config = PeftConfig.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model using the loaded configuration and other parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig.base_model_name_or_path,\n",
    "\treturn_dict=True,\n",
    "\tquantization_config=bnb_config,\n",
    "\tdevice_map=\"auto\",\n",
    "\ttrust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(chapter : str) -> str:\n",
    "    \n",
    "    \n",
    "\tprompt =  f\"\"\"\n",
    "    \"Summarize the following : \\n\" {chapter}\n",
    "    \\n\\nSummary: \n",
    "    \"\"\".strip()\n",
    "\tencoding = tokenizer(prompt, return_tensors = \"pt\").to(DEVICE)\n",
    "\t#with torch.inference_mode():\n",
    "    with torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\tinput_ids=encoding.input_ids,\n",
    "\t\t\tattention_mask=encoding.attention_mask,\n",
    "\t\t\tgeneration_config=generation_config,\n",
    "\t\t)\n",
    "\n",
    "\tresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t#assistant_start =  \"<assistant>:\"\n",
    "\t#response_start = response.find(assistant_start)\n",
    "\t#return response[response_start + len(assistant_start) : ].strip()\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt\n",
    "\n",
    "chapter = \"text to be summarised\"\n",
    "print (generate_response(chapter))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
