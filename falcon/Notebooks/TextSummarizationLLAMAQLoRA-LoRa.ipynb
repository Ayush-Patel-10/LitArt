{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 17:51:42.723235: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 17:51:44.523739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from datetime import date\n",
    "\n",
    "import torch\n",
    "#import lightning as L\n",
    "#from lightning.pytorch.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "#Transformers\n",
    "import transformers\n",
    "import tqdm\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from transformers import AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "#from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "#Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "#PEFT\n",
    "from peft import LoraConfig\n",
    "from peft import PeftConfig\n",
    "from peft import PeftModel\n",
    "from peft import get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to print the number of trainable parameters in the given model\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable %: {100 * trainable_params / all_param}\")\n",
    "\n",
    "def tokenize_input(df,tokenizer,tokenizer_chapter_max_length,tokenizer_summary_max_length):\n",
    "\n",
    "    prompt_start = \"Summarize the following : \\n\"\n",
    "    prompt_end = \"\\n\\nSummary:\"\n",
    "\n",
    "    prompt = [prompt_start + dialogue + prompt_end for dialogue in df[\"chapter\"]]\n",
    "\n",
    "    df[\"input_ids\"] = tokenizer(prompt, max_length=tokenizer_chapter_max_length , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "    df[\"labels\"] = tokenizer(df[\"summary_text\"],max_length=tokenizer_summary_max_length , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/work/LitArt/nair/cache/\" \n",
    "log_path = \"/work/LitArt/nair/outdir/\"\n",
    "\n",
    "tokenizer_chapter_max_length = 1024\n",
    "tokenizer_summary_max_length = 256\n",
    "model = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "\n",
    "log_path = log_path+model.replace(\"/\",\"-\")+\"-\" +str(today)+\"-\"+time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "#logger = TensorBoardLogger(log_path, name=\"my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a91ebbd2864c9bbe386d1826f8409e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 40554496 || All params: 3540967424 || Trainable %: 1.1452942414869276\n",
      "Trainable params: 40554496 || All params: 3540967424 || Trainable %: 1.1452942414869276\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "#Bits and Bytes config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "load_in_4bit=True, #4bit quantizaition - load_in_4bit is used to load models in 4-bit quantization \n",
    "bnb_4bit_use_double_quant=True, #nested quantization technique for even greater memory efficiency without sacrificing performance. This technique has proven beneficial, especially when fine-tuning large models\n",
    "bnb_4bit_quant_type=\"nf4\", #quantization type used is 4 bit Normal Float Quantization- The NF4 data type is designed for weights initialized using a normal distribution\n",
    "bnb_4bit_compute_dtype=torch.bfloat16, #modify the data type used during computation. This can result in speed improvements. \n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model,\n",
    "                                                    device_map=\"auto\",\n",
    "                                                    trust_remote_code=True,\n",
    "                                                    quantization_config=bnb_config,\n",
    "                                                    cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "\n",
    "# Enable gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce the memory consumption during the backward pas. Instead of storing all intermediate activations in the forward pass (which is what's typically done to compute gradients in the backward pass), gradient checkpointing stores only a subset of them\n",
    "model.gradient_checkpointing_enable() \n",
    "\n",
    "# Prepare the model for k-bit training . Applies some preprocessing to the model to prepare it for training.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "#If only targeting attention blocks of the model\n",
    "#target_modules = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "#If targeting all linear layers\n",
    "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "\n",
    "    \n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules = target_modules,\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config)\n",
    "\n",
    "#base_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print the number of trainable parameters in the model\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(log_path, exist_ok=True)\n",
    "file_path = os.path.join(log_path, 'number_of_trainable_para.txt')  \n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(str(print_trainable_parameters(model)))\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('csv', \n",
    "                    data_files={\n",
    "                        'train': \"/work/LitArt/data/chunked_dataset/train_dataset_with_summaries.csv\",\n",
    "                        'test': \"/work/LitArt/data/chunked_dataset/test_dataset_with_summaries.csv\",\n",
    "                        'val':\"/work/LitArt/data/chunked_dataset/validation_dataset_with_summaries.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 10668\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 1614\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 1548\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bec8ad33604ea4ba4f112d36635cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = data[\"train\"].shuffle().map(tokenize_input, batched=True, fn_kwargs={\"tokenizer\": tokenizer, \"tokenizer_chapter_max_length\": tokenizer_chapter_max_length,\"tokenizer_summary_max_length\":tokenizer_summary_max_length})\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['chapter', 'human_summary', '__index_level_0__', 'summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "#Training Parameters\n",
    "batch_size = 1\n",
    "epochs= 5\n",
    "output_dir = f\"llama-7b-qlora-Capstone-project\"\n",
    "per_device_train_batch_size = batch_size\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'adamw_hf' #\"paged_adamw_32bit\" #\"paged_adamw_8bit\"\n",
    "save_steps = 10\n",
    "save_total_limit=3\n",
    "logging_steps = 30\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 2000\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"cosine\" #\"cosine\"\n",
    "epochs=1, \n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=log_path,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=epochs, \n",
    "    #save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    save_strategy='epoch',\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_checkpointing=True,\n",
    "    #push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "def formatting_func(example):\n",
    "    text = f\"### USER: Summarize the following text : {example['chapter']}\\n### ASSISTANT: {example['summary_text']}\"\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420d86dd14864047956a64706cc2238b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=data[\"train\"],\n",
    "    packing=True,\n",
    "    #dataset_text_field=\"id\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=1024,\n",
    "    formatting_func=formatting_func,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hyperparameters(log_path, quantization_config, lora_config , training_arguments):\n",
    "    import os\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(log_path, 'hyperparameters.txt')  \n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(str(quantization_config))\n",
    "        file.write(str(lora_config))\n",
    "        file.write(str(training_arguments))\n",
    "        \n",
    "    \n",
    "save_hyperparameters(log_path, bnb_config , lora_config , training_arguments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  62/2000 02:54 < 1:33:46, 0.34 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.907500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.715800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train() #  [ 861/1000 1:30:16 < 14:36, 0.16 it/s, Epoch 5.76/7] at 348pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "#model_dir = '/work/LitArt/nair/outdir/meta-llama-Llama-2-7b-hf-2024-03-21-14:17:13'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=quantization_config,\n",
    "    #adapter_kwargs={\"revision\": \"09487e6ffdcc75838b10b6138b6149c36183164e\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_response(chapter : str) -> str:\n",
    "    prompt =  f\"\"\"### USER: Summarize the following text : ' {chapter}' ### Assistant:  \"\"\".strip()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
    "    outputs = model.generate(inputs.input_ids, max_new_tokens=500, do_sample=False)\n",
    "    return(tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
    "\n",
    "\n",
    "'''\n",
    "\tencoding = tokenizer(prompt, return_tensors = \"pt\").to(DEVICE)\n",
    "\t#with torch.inference_mode():\n",
    "    with torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\tinput_ids=encoding.input_ids,\n",
    "\t\t\tattention_mask=encoding.attention_mask,\n",
    "\t\t\tgeneration_config=generation_config,\n",
    "\t\t)\n",
    "\n",
    "\tresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t#assistant_start =  \"<assistant>:\"\n",
    "\t#response_start = response.find(assistant_start)\n",
    "\t#return response[response_start + len(assistant_start) : ].strip()\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "chapter = '''In the dim light of the old library, Anna's fingers traced the edges of a leather-bound book, its title embossed in gold but faded with time. She could hear the distant echo of the storm outside, a reminder of the world she had momentarily escaped. Around her, shelves towered like ancient guardians, filled with stories waiting to be told.\n",
    "\n",
    "As she flipped through the pages, a loose sheet of paper slipped out and floated to the ground. It was a map, old and hand-drawn, marking a location in the heart of the city that Anna couldn't recall ever hearing about. Her curiosity piqued, she decided then that the storm would not deter her adventure. Tucking the map into her coat, she stepped out into the rain, the library door closing with a soft thud behind her.\n",
    "\n",
    "Navigating through the slick streets, Anna's mind raced with possibilities of what she might find. The map led her to an alley she had passed a thousand times but never noticed. Hidden away was a door, as if waiting for her all these years. She pushed it open, the creak of the hinges echoing into the unseen depths beyond.\n",
    "\n",
    "Inside, the air was thick with the scent of old books and whispered secrets. A single lamp illuminated a room that seemed out of place and time, filled with artifacts and tomes that whispered of magic and mystery. At the center, a figure turned from a cluttered table, their eyes meeting Anna's with a mix of surprise and expectation.\n",
    "\n",
    "\"You've found your way,\" the figure began, their voice a blend of warmth and intrigue. \"But remember, what you seek also seeks you. The journey ahead is yours alone to embrace.\"\n",
    "\n",
    "Before Anna could reply, the world around her began to blur, the edges of reality seeming to fray. The room, the figure, and the artifacts faded into a swirl of colors and whispers.\n",
    "\n",
    "And then, she was standing back in the alley, the door now just a wall, the map in her hand turned to dust. The rain had stopped, and the city seemed unaware of the journey she had just embarked upon. Anna looked around, the weight of the adventure settling in her heart, knowing her story was far from over.\n",
    "'''\n",
    "print(generate_response(chapter))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d= pd.DataFrame(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(log_path, 'log.csv')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = os.path.join(log_path, 'summary.txt')  \n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    file.write(str(generate_response(chapter)))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"val\"]['chapter'][100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (generate_response(data[\"val\"]['chapter'][100]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
