{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 18:06:33.885583: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 18:06:35.684990: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from datetime import date\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "#Transformers\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from transformers import AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "#Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "#PEFT\n",
    "from peft import LoraConfig\n",
    "from peft import PeftConfig\n",
    "from peft import PeftModel\n",
    "from peft import get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to print the number of trainable parameters in the given model\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable %: {100 * trainable_params / all_param}\")\n",
    "\n",
    "def tokenize_input(df,tokenizer,tokenizer_chapter_max_length,tokenizer_summary_max_length):\n",
    "\n",
    "    prompt_start = \"Summarize the following : \\n\"\n",
    "    prompt_end = \"\\n\\nSummary:\"\n",
    "\n",
    "    prompt = [prompt_start + dialogue + prompt_end for dialogue in df[\"chapter\"]]\n",
    "\n",
    "    df[\"input_ids\"] = tokenizer(prompt, max_length=tokenizer_chapter_max_length , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "    df[\"labels\"] = tokenizer(df[\"summary_text\"],max_length=tokenizer_summary_max_length , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/work/LitArt/nair/cache/\" \n",
    "log_path = \"/work/LitArt/nair/outdir/\"\n",
    "\n",
    "tokenizer_chapter_max_length = 1024\n",
    "tokenizer_summary_max_length = 256\n",
    "\n",
    "#base_model_name = \"tiiuae/falcon-7b\"\n",
    "#tokenizer_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "model = \"tiiuae/falcon-40b-instruct\"\n",
    "tokenizer_name = \"tiiuae/falcon-40b-instruct\"\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "\n",
    "log_path = log_path+model.replace(\"/\",\"-\")+\"-\" +str(today)+\"-\"+time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "#logger = TensorBoardLogger(log_path, name=\"my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/LitArt/nair/outdir/tiiuae-falcon-40b-instruct-2024-03-21-18:06:41'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585df5897d9240dbb6951e768cc143ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the 7b llama model\n",
    "#model_id = \"tiiuae/falcon-7b\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model, \n",
    "                                                  quantization_config=quantization_config, \n",
    "                                                  cache_dir=cache_dir,\n",
    "                                                  device_map=\"auto\",\n",
    "                                                  trust_remote_code=True,\n",
    "                                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 18800640 || All params: 20937777152 || Trainable %: 0.08979291289383191\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,cache_dir=cache_dir)\n",
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Enable gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce the memory consumption during the backward pas. Instead of storing all intermediate activations in the forward pass (which is what's typically done to compute gradients in the backward pass), gradient checkpointing stores only a subset of them\n",
    "model.gradient_checkpointing_enable() \n",
    "\n",
    "# Prepare the model for k-bit training . Applies some preprocessing to the model to prepare it for training.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=18, #The rank of decomposition r is << min(d,k). The default of r is 8.\n",
    "    lora_alpha=32,#∆W is scaled by α/r where α is a constant. When optimizing with Adam, tuning α is similar as tuning the learning rate.\n",
    "    target_modules=[\"query_key_value\"], #Modules to Apply LoRA to target_modules. You can select specific modules to fine-tune.\n",
    "    lora_dropout=0.05,#Dropout Probability for LoRA Layers #to reduce overfitting\n",
    "    bias=\"none\", #Bias Type for Lora. Bias can be ‘none’, ‘all’ or ‘lora_only’. If ‘all’ or ‘lora_only’, the corresponding biases will be updated during training. \n",
    "    task_type= \"CAUSAL_LM\", #Task Type\n",
    "    )\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print the number of trainable parameters in the model\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('csv', \n",
    "                    data_files={\n",
    "                        'train': \"/work/LitArt/data/chunked_dataset/train_dataset_with_summaries.csv\",\n",
    "                        'test': \"/work/LitArt/data/chunked_dataset/test_dataset_with_summaries.csv\",\n",
    "                        'val':\"/work/LitArt/data/chunked_dataset/validation_dataset_with_summaries.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 10668\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 1614\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 1548\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53a91583dc24250a9212d358e290d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = data[\"train\"].shuffle().map(tokenize_input, batched=True, fn_kwargs={\"tokenizer\": tokenizer, \"tokenizer_chapter_max_length\": tokenizer_chapter_max_length,\"tokenizer_summary_max_length\":tokenizer_summary_max_length})\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['chapter', 'human_summary', '__index_level_0__', 'summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "#Training Parameters\n",
    "batch_size = 2\n",
    "epochs = 3\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size = batch_size ,     # Specifies the batch size for training on each device (GPU).\n",
    "    #auto_find_batch_size=True,      # Uncommenting this would let the library automatically find an optimal batch size.\n",
    "    gradient_accumulation_steps=2,   # Number of forward and backward passes to accumulate gradients before performing an optimizer step.\n",
    "    # This effectively multiplies the batch size by this number without increasing memory usage.\n",
    "    num_train_epochs=epochs,              # Specifies the total number of training epochs.\n",
    "    learning_rate=1e-5,              # Specifies the learning rate for the optimizer.\n",
    "    fp16=True,     # Enables mixed precision training (fp16) which can speed up training and reduce memory usage.\n",
    "    save_total_limit=3,              # Limits the total number of model checkpoints saved. Only the last 3 checkpoints are saved.\n",
    "    logging_steps=10,                 # Specifies how often to log training updates. \n",
    "    output_dir=log_path ,          # Directory where the model checkpoints and training outputs will be saved.\n",
    "    max_steps = 400 ,                 # Limits the total number of training steps. Training will stop after 80 steps regardless of epochs.\n",
    "    save_strategy='epoch',    # Uncommenting this would change the strategy for saving model checkpoints. 'epoch' means save after each epoch.\n",
    "    optim=\"paged_adamw_8bit\",     # Specifies the optimizer to use. it's set to a specific variant of AdamW.\n",
    "    lr_scheduler_type = 'cosine',     # Specifies the learning rate scheduler type. 'cosine' means it uses cosine annealing.\n",
    "    warmup_ratio = 0.05,           # Specifies the ratio of total steps for the learning rate warmup phase.\n",
    "    max_grad_norm=0.3,\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hyperparameters(log_path, quantization_config, lora_config , training_args):\n",
    "    import os\n",
    "    os.makedirs(log_path, exist_ok=True)\n",
    "    \n",
    "    file_path = os.path.join(log_path, 'hyperparameters.txt')  \n",
    "    \n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(str(quantization_config))\n",
    "        file.write(str(lora_config))\n",
    "        file.write(str(training_args))\n",
    "        \n",
    "    \n",
    "save_hyperparameters(log_path, quantization_config , lora_config , training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65/400 09:04 < 48:15, 0.12 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.656600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.837900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.584000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class LossLoggingCallback(TrainerCallback):\n",
    "    \"A custom callback that logs the loss after each epoch.\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.losses = []\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        # Log the average training loss of the last epoch\n",
    "        self.losses.append(state.log_history[-1]['loss'])\n",
    "\n",
    "# Instantiate your callback\n",
    "loss_logging_callback = LossLoggingCallback()\n",
    "\n",
    "# Add your callback to the list of callbacks in the Trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    callbacks=[loss_logging_callback]  # Add your custom callback here\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # Adjust as per your needs\n",
    "\n",
    "# Assuming training is performed here\n",
    "trainer.train()\n",
    "\n",
    "# After training, plot the loss vs. epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_logging_callback.losses, label='Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Save the plot to the specified log_path folder\n",
    "#plt.savefig(f'{log_path}/loss_vs_epochs.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainer.train() # [276/500 39:37 < 32:23, 0.12 it/s, Epoch 0.10/1] at 3pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(log_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration for the trained model\n",
    "config = PeftConfig.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model using the loaded configuration and other parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig.base_model_name_or_path,\n",
    "\treturn_dict=True,\n",
    "\tquantization_config=quantization_config,\n",
    "\tdevice_map=\"auto\",\n",
    "\ttrust_remote_code=True,\n",
    "    cache_dir=cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  PeftModel.from_pretrained(model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(chapter : str) -> str:    \n",
    "    prompt =  f'''\n",
    "    \"Summarize the following : \\n\" {chapter}\n",
    "    \"\\n\\nSummary:\" \n",
    "    '''.strip()\n",
    "    DEVICE  = \"cuda\"\n",
    "    encoding = tokenizer(prompt, return_tensors = \"pt\").to(DEVICE)\n",
    "    #with torch.inference_mode():\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt\n",
    "\n",
    "chapter = '''In a village where the mountains kissed the clouds and the rivers sang to the valleys, there lived a boy named Idris. Idris had the peculiar ability to understand the language of the wind. It was a gift that had been passed down through his family for generations, but in Idris, it found its most curious student.\n",
    "\n",
    "Each morning, Idris would climb to the highest peak, sit among the whispers of the passing breezes, and listen. The wind spoke of distant lands, of the secrets of the forest, and of the tales of the creatures that walked within it. Idris's favorite stories were those of the Guardians of the Forest, mythical beings said to protect the balance between nature and the world of men.\n",
    "\n",
    "One day, a tempest unlike any other approached the village. The wind's voice was frantic, warning of a darkness that sought to devour the forest and everything within it. Idris understood what he had to do. He remembered the tales of the Guardians and knew that if he could find them, they could save his home.\n",
    "\n",
    "With nothing but the clothes on his back and the courage in his heart, Idris ventured into the forest. The wind guided him, whispering paths through the twisting undergrowth and overgrown trails. After days of journeying deeper than any villager had dared, Idris found the heart of the forest. It was there, in a clearing bathed in moonlight, that he met the Guardians.\n",
    "\n",
    "They were not what he expected. The Guardians were the forest itself — the trees, the rivers, the stones, and the wind. They spoke in a chorus of natural harmony, and Idris understood them. He pleaded for their help, telling them of the impending darkness.\n",
    "\n",
    "The Guardians listened and then spoke in a voice like the rustling of leaves. \"The darkness you speak of is born from the hearts of men,\" they said. \"It cannot be fought with force but with understanding. Return to your people, Idris. Teach them to listen as you have listened.\"\n",
    "\n",
    "Idris returned to his village as the storm broke upon the mountains. He told them of the Guardians, of the language of the wind, and of the darkness that was born from their own actions. At first, they did not understand, but Idris did not give up. He showed them how to listen, how to care for the land that had cared for them.\n",
    "\n",
    "In time, the village changed. The people learned to live in harmony with the land, to listen to the wind, and to respect the balance of nature. The darkness receded, the forest flourished, and Idris's village prospered.\n",
    "\n",
    "Idris grew old, but he continued to climb to the peak each morning, to listen to the wind, and to teach others to do the same. And though the story of the boy who spoke to the wind became a legend, the lesson it taught remained: to listen, to understand, and to respect the voices of the world around us.\n",
    "'''\n",
    "\n",
    "print (generate_response(chapter))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
