{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 11:12:07.712551: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-21 11:12:11.902083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from datetime import date\n",
    "\n",
    "import torch\n",
    "\n",
    "#Transformers\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from transformers import AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "#Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "#PEFT\n",
    "from peft import LoraConfig\n",
    "from peft import PeftConfig\n",
    "from peft import PeftModel\n",
    "from peft import get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to print the number of trainable parameters in the given model\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable %: {100 * trainable_params / all_param}\")\n",
    "\n",
    "def tokenize_input(df,tokenizer,tokenizer_chapter_max_length,tokenizer_summary_max_length):\n",
    "\n",
    "    prompt_start = \"Summarize the following : \\n\"\n",
    "    prompt_end = \"\\n\\nSummary:\"\n",
    "\n",
    "    prompt = [prompt_start + dialogue + prompt_end for dialogue in df[\"chapter\"]]\n",
    "\n",
    "    df[\"input_ids\"] = tokenizer(prompt, max_length=tokenizer_chapter_max_length , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "    df[\"labels\"] = tokenizer(df[\"summary_text\"],max_length=tokenizer_summary_max_length , padding=\"max_length\" , truncation=True , return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/work/LitArt/nair/cache/\" \n",
    "log_path = \"/work/LitArt/nair/outdir/\"\n",
    "\n",
    "tokenizer_chapter_max_length = 1024\n",
    "tokenizer_summary_max_length = 256\n",
    "base_model_name = \"tiiuae/falcon-7b\"\n",
    "tokenizer_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "#Training Parameters\n",
    "batch_size = 2\n",
    "epochs = 1\n",
    "log_path = log_path+base_model_name.replace(\"/\",\"-\")+\"-\" +str(today)+\"-\"+time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "#logger = TensorBoardLogger(log_path, name=\"my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610f0fb4e4a44163b12c3c451dd621bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the 7b llama model\n",
    "model_id = \"tiiuae/falcon-7b\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                                  quantization_config=quantization_config, \n",
    "                                                  cache_dir=cache_dir,\n",
    "                                                  device_map=\"auto\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 5308416 || All params: 3614053248 || Trainable %: 0.1468826172646364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,cache_dir=cache_dir)\n",
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Enable gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce the memory consumption during the backward pas. Instead of storing all intermediate activations in the forward pass (which is what's typically done to compute gradients in the backward pass), gradient checkpointing stores only a subset of them\n",
    "base_model.gradient_checkpointing_enable() \n",
    "\n",
    "# Prepare the model for k-bit training . Applies some preprocessing to the model to prepare it for training.\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=18, #The rank of decomposition r is << min(d,k). The default of r is 8.\n",
    "    lora_alpha=32,#∆W is scaled by α/r where α is a constant. When optimizing with Adam, tuning α is similar as tuning the learning rate.\n",
    "    target_modules=[\"query_key_value\"], #Modules to Apply LoRA to target_modules. You can select specific modules to fine-tune.\n",
    "    lora_dropout=0.05,#Dropout Probability for LoRA Layers #to reduce overfitting\n",
    "    bias=\"none\", #Bias Type for Lora. Bias can be ‘none’, ‘all’ or ‘lora_only’. If ‘all’ or ‘lora_only’, the corresponding biases will be updated during training. \n",
    "    task_type= \"CAUSAL_LM\", #Task Type\n",
    "    )\n",
    "\n",
    "base_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print the number of trainable parameters in the model\n",
    "print_trainable_parameters(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('csv', \n",
    "                    data_files={\n",
    "                        'train': \"/work/LitArt/data/chunked_dataset/train_dataset_with_summaries.csv\",\n",
    "                        'test': \"/work/LitArt/data/chunked_dataset/test_dataset_with_summaries.csv\",\n",
    "                        'val':\"/work/LitArt/data/chunked_dataset/validation_dataset_with_summaries.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 10668\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 1614\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['chapter', 'human_summary', '__index_level_0__', 'summary_text'],\n",
       "        num_rows: 1548\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad1ee96f74c4ff885a562b765676718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = data[\"train\"].shuffle().map(tokenize_input, batched=True, fn_kwargs={\"tokenizer\": tokenizer, \"tokenizer_chapter_max_length\": tokenizer_chapter_max_length,\"tokenizer_summary_max_length\":tokenizer_summary_max_length})\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['chapter', 'human_summary', '__index_level_0__', 'summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size = batch_size ,     # Specifies the batch size for training on each device (GPU).\n",
    "    #auto_find_batch_size=True,      # Uncommenting this would let the library automatically find an optimal batch size.\n",
    "    gradient_accumulation_steps=2,   # Number of forward and backward passes to accumulate gradients before performing an optimizer step.\n",
    "    # This effectively multiplies the batch size by this number without increasing memory usage.\n",
    "    num_train_epochs=epochs,              # Specifies the total number of training epochs.\n",
    "    learning_rate=2e-4,              # Specifies the learning rate for the optimizer.\n",
    "    fp16=True,     # Enables mixed precision training (fp16) which can speed up training and reduce memory usage.\n",
    "    save_total_limit=3,              # Limits the total number of model checkpoints saved. Only the last 3 checkpoints are saved.\n",
    "    logging_steps=10,                 # Specifies how often to log training updates. \n",
    "    output_dir=log_path ,          # Directory where the model checkpoints and training outputs will be saved.\n",
    "    max_steps = 200 ,                 # Limits the total number of training steps. Training will stop after 80 steps regardless of epochs.\n",
    "    save_strategy='epoch',    # Uncommenting this would change the strategy for saving model checkpoints. 'epoch' means save after each epoch.\n",
    "    optim=\"paged_adamw_8bit\",     # Specifies the optimizer to use. it's set to a specific variant of AdamW.\n",
    "    lr_scheduler_type = 'cosine',     # Specifies the learning rate scheduler type. 'cosine' means it uses cosine annealing.\n",
    "    warmup_ratio = 0.05,           # Specifies the ratio of total steps for the learning rate warmup phase.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 06:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.291000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.108500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.138200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.174200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.113600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.216200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.218500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=3.1530834293365477, metrics={'train_runtime': 372.1503, 'train_samples_per_second': 2.15, 'train_steps_per_second': 0.537, 'total_flos': 3.25954440265728e+16, 'train_loss': 3.1530834293365477, 'epoch': 0.07})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=base_model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "base_model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save_pretrained(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configuration for the trained model\n",
    "config = PeftConfig.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834737ed053a42958f7107b4b417d174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- configuration_py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c128ed61f7c6423eb1741f8d00f7fae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- modeling_py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ded28eda6994e2cb050719e95e820d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/16.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0102ba01c8491290ca1f7ba28e114b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "542d82fd29814ca1b6153c75cc7baffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0457b7a8e84d4aa28dbe25ded9e64b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d586f5666c246c0898c350aad788bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05dac94000ff4e2b8795358fe934f442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the trained model using the loaded configuration and other parameters\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig.base_model_name_or_path,\n",
    "\treturn_dict=True,\n",
    "\tquantization_config=quantization_config,\n",
    "\tdevice_map=\"auto\",\n",
    "\ttrust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9681fdf96750449aaf2237b8e818217d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64fb33cedf04453ead638b35be6dcef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a232ba2b4c6044ad8a4901bb732f56da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the padding token of the tokenizer to its end-of-sentence token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 200\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:10\u001b[0;36m\u001b[0m\n\u001b[0;31m    with torch.no_grad():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def generate_response(chapter : str) -> str:\n",
    "    \n",
    "    \n",
    "\tprompt =  f\"\"\"\n",
    "    \"Summarize the following : \\n\" {chapter}\n",
    "    \\n\\nSummary: \n",
    "    \"\"\".strip()\n",
    "\tencoding = tokenizer(prompt, return_tensors = \"pt\").to(DEVICE)\n",
    "\t#with torch.inference_mode():\n",
    "    with torch.no_grad():\n",
    "\t\toutputs = model.generate(\n",
    "\t\t\tinput_ids=encoding.input_ids,\n",
    "\t\t\tattention_mask=encoding.attention_mask,\n",
    "\t\t\tgeneration_config=generation_config,\n",
    "\t\t)\n",
    "\n",
    "\tresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\t#assistant_start =  \"<assistant>:\"\n",
    "\t#response_start = response.find(assistant_start)\n",
    "\t#return response[response_start + len(assistant_start) : ].strip()\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt\n",
    "\n",
    "chapter = \"text to be summarised\"\n",
    "print (generate_response(chapter))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
